"""
Enterprise Project Migration and Environment Transfer System
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Enterprise ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
"""

import os
import json
import shutil
import zipfile
import tarfile
import subprocess
import platform
import hashlib
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
import yaml
import io

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, SpinnerColumn
from rich.prompt import Prompt, Confirm
from rich.syntax import Syntax

console = Console()

class EnterpriseProjectMigrator:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Enterprise"""
    
    def __init__(self, project_path: str = "."):
        self.project_path = Path(project_path).resolve()
        self.migration_data = {}
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.migration_log = []
        
    def _log(self, message: str, level: str = "INFO"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log ‡∏Å‡∏≤‡∏£ migration"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        self.migration_log.append(log_entry)
        
        if level == "SUCCESS":
            console.print(f"‚úÖ {message}", style="green")
        elif level == "ERROR":
            console.print(f"‚ùå {message}", style="red")
        elif level == "WARNING":
            console.print(f"‚ö†Ô∏è {message}", style="yellow")
        else:
            console.print(f"‚ÑπÔ∏è {message}", style="blue")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì hash ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
        
    def analyze_project(self) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
        
        self._log("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå...")
        
        analysis = {
            'project_info': {
                'name': self.project_path.name,
                'path': str(self.project_path),
                'size_bytes': 0,
                'file_count': 0,
                'created': datetime.now().isoformat(),
                'platform': platform.system(),
                'python_version': platform.python_version()
            },
            'dependencies': {
                'python_packages': [],
                'requirements_files': [],
                'conda_env': None,
                'system_packages': []
            },
            'project_structure': {
                'directories': [],
                'key_files': [],
                'data_files': [],
                'model_files': [],
                'config_files': []
            },
            'tracking_components': {
                'mlflow_data': [],
                'wandb_data': [],
                'local_tracking': [],
                'artifacts': []
            },
            'deployment_configs': {
                'docker_files': [],
                'k8s_configs': [],
                'cloud_configs': [],
                'env_files': []
            }
        }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå
        for root, dirs, files in os.walk(self.project_path):
            root_path = Path(root)
            rel_path = root_path.relative_to(self.project_path)
            
            if rel_path != Path('.'):
                analysis['project_structure']['directories'].append(str(rel_path))
            
            for file in files:
                file_path = root_path / file
                rel_file_path = file_path.relative_to(self.project_path)
                file_size = file_path.stat().st_size
                analysis['project_info']['size_bytes'] += file_size
                analysis['project_info']['file_count'] += 1
                
                # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
                if file.endswith(('.py', '.ipynb', '.R', '.sql')):
                    analysis['project_structure']['key_files'].append(str(rel_file_path))
                elif file.endswith(('.csv', '.json', '.parquet', '.pkl', '.h5')):
                    analysis['project_structure']['data_files'].append(str(rel_file_path))
                elif file.endswith(('.joblib', '.pkl', '.h5', '.onnx', '.pt', '.pth')):
                    analysis['project_structure']['model_files'].append(str(rel_file_path))
                elif file.endswith(('.yaml', '.yml', '.ini', '.conf', '.env')):
                    analysis['project_structure']['config_files'].append(str(rel_file_path))
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå dependencies
        self._analyze_dependencies(analysis)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå tracking components
        self._analyze_tracking_components(analysis)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå deployment configs
        self._analyze_deployment_configs(analysis)
        
        self.migration_data = analysis
        self._log(f"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {analysis['project_info']['file_count']} ‡πÑ‡∏ü‡∏•‡πå, ‡∏Ç‡∏ô‡∏≤‡∏î {analysis['project_info']['size_bytes'] / (1024*1024):.1f} MB", "SUCCESS")
        
        return analysis
    
    def _analyze_dependencies(self, analysis: Dict[str, Any]):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå dependencies"""
        
        # ‡∏´‡∏≤ requirements files
        req_files = ['requirements.txt', 'tracking_requirements.txt', 'dev-requirements.txt', 'environment.yml']
        for req_file in req_files:
            req_path = self.project_path / req_file
            if req_path.exists():
                analysis['dependencies']['requirements_files'].append(req_file)
        
        # ‡∏´‡∏≤ conda environment
        conda_files = ['environment.yml', 'conda.yaml']
        for conda_file in conda_files:
            conda_path = self.project_path / conda_file
            if conda_path.exists():
                analysis['dependencies']['conda_env'] = conda_file
        
        # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Python packages
        try:
            result = subprocess.run(['pip', 'list', '--format=json'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                packages = json.loads(result.stdout)
                analysis['dependencies']['python_packages'] = packages
        except Exception:
            pass
    
    def _analyze_tracking_components(self, analysis: Dict[str, Any]):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á tracking system"""
        
        # MLflow data
        mlflow_dirs = ['mlruns', 'enterprise_mlruns']
        for mlflow_dir in mlflow_dirs:
            mlflow_path = self.project_path / mlflow_dir
            if mlflow_path.exists():
                analysis['tracking_components']['mlflow_data'].append(mlflow_dir)
        
        # Local tracking data
        tracking_dirs = ['tracking', 'enterprise_tracking', 'experiments']
        for tracking_dir in tracking_dirs:
            tracking_path = self.project_path / tracking_dir
            if tracking_path.exists():
                analysis['tracking_components']['local_tracking'].append(tracking_dir)
        
        # Artifacts
        artifact_dirs = ['artifacts', 'models', 'reports']
        for artifact_dir in artifact_dirs:
            artifact_path = self.project_path / artifact_dir
            if artifact_path.exists():
                analysis['tracking_components']['artifacts'].append(artifact_dir)
    
    def _analyze_deployment_configs(self, analysis: Dict[str, Any]):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö deployment"""
        
        # Docker files
        docker_files = ['Dockerfile', 'docker-compose.yml', 'docker-compose.yaml']
        for docker_file in docker_files:
            docker_path = self.project_path / docker_file
            if docker_path.exists():
                analysis['deployment_configs']['docker_files'].append(docker_file)
        
        # Kubernetes configs
        k8s_path = self.project_path / 'k8s'
        if k8s_path.exists():
            analysis['deployment_configs']['k8s_configs'] = [str(f.relative_to(self.project_path)) 
                                                           for f in k8s_path.rglob('*.yaml')]
        
        # Environment files
        env_files = ['.env', '.env.example', '.env.production', '.env.staging']
        for env_file in env_files:
            env_path = self.project_path / env_file
            if env_path.exists():
                analysis['deployment_configs']['env_files'].append(env_file)
        
    def create_migration_package(self, output_path: Optional[str] = None) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á migration package ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"""
        
        if output_path is None:
            output_path = f"project_migration_{self.timestamp}.zip"
            
        console.print(f"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Migration Package...", style="bold blue")
        
        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            TimeElapsedColumn(),
        ) as progress:
            
            # Task definitions
            task1 = progress.add_task("üìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå...", total=100)
            task2 = progress.add_task("üì¶ ‡∏™‡∏£‡πâ‡∏≤‡∏á environment snapshot...", total=100)
            task3 = progress.add_task("üíæ ‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå...", total=100)
            task4 = progress.add_task("üìã ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£...", total=100)
            
            # Step 1: Analyze project
            self._analyze_project()
            progress.update(task1, advance=100)
            
            # Step 2: Create environment snapshot
            self._create_environment_snapshot()
            progress.update(task2, advance=100)
            
            # Step 3: Create migration package
            self._create_zip_package(output_path)
            progress.update(task3, advance=100)
            
            # Step 4: Create documentation
            self._create_migration_docs()
            progress.update(task4, advance=100)
            
        console.print(f"‚úÖ Migration Package ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à: {output_path}", style="bold green")
        return output_path
    
    def _analyze_project(self):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå"""
        
        analysis = {
            "project_info": {
                "name": self.project_path.name,
                "path": str(self.project_path),
                "size_mb": self._get_directory_size(self.project_path) / (1024 * 1024),
                "created_date": self.timestamp,
                "platform": platform.platform(),
                "python_version": platform.python_version()
            },
            "file_structure": self._get_file_structure(),
            "dependencies": self._analyze_dependencies(),
            "configurations": self._find_config_files(),
            "data_files": self._find_data_files(),
            "models": self._find_model_files(),
            "tracking_data": self._analyze_tracking_data()
        }
        
        self.migration_data["analysis"] = analysis
        
        # Save analysis
        with open(self.project_path / "migration_analysis.json", "w", encoding="utf-8") as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    def _create_environment_snapshot(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á snapshot ‡∏Ç‡∏≠‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°"""
        
        env_data = {
            "system_info": self._get_system_info(),
            "python_info": self._get_python_info(),
            "packages": self._get_installed_packages(),
            "environment_variables": self._get_relevant_env_vars(),
            "conda_info": self._get_conda_info(),
            "git_info": self._get_git_info()
        }
        
        # Create requirements files
        self._create_requirements_files()
        
        # Create environment.yml for conda
        self._create_conda_environment()
        
        # Create Dockerfile
        self._create_dockerfile()
        
        # Create docker-compose.yml
        self._create_docker_compose()
        
        self.migration_data["environment"] = env_data
        
        # Save environment data
        with open(self.project_path / "environment_snapshot.json", "w", encoding="utf-8") as f:
            json.dump(env_data, f, indent=2, ensure_ascii=False)
    
    def _create_requirements_files(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå requirements ‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
        
        try:
            # pip freeze
            result = subprocess.run(["pip", "freeze"], capture_output=True, text=True)
            with open(self.project_path / "requirements_freeze.txt", "w") as f:
                f.write(result.stdout)
                
            # pipreqs (if available)
            try:
                subprocess.run(["pipreqs", str(self.project_path), "--force"], 
                             capture_output=True, check=True)
            except:
                pass
                
        except Exception as e:
            console.print(f"‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á requirements ‡πÑ‡∏î‡πâ: {e}", style="yellow")
    
    def _create_conda_environment(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå environment.yml ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö conda"""
        
        try:
            result = subprocess.run(["conda", "env", "export"], capture_output=True, text=True)
            if result.returncode == 0:
                with open(self.project_path / "environment.yml", "w") as f:
                    f.write(result.stdout)
        except:
            # Create basic environment.yml
            env_yml = {
                "name": f"{self.project_path.name}_env",
                "channels": ["conda-forge", "defaults"],
                "dependencies": [
                    f"python={platform.python_version()}",
                    "pip",
                    {"pip": ["mlflow", "rich", "pandas", "numpy", "scikit-learn"]}
                ]
            }
            
            with open(self.project_path / "environment.yml", "w") as f:
                yaml.dump(env_yml, f, default_flow_style=False)
    
    def _create_dockerfile(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Dockerfile"""
        
        dockerfile_content = f"""# Dockerfile for {self.project_path.name}
# Generated on {self.timestamp}

FROM python:{platform.python_version()}-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    git \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements*.txt ./
COPY environment.yml ./

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements_freeze.txt

# Copy project files
COPY . .

# Create necessary directories
RUN mkdir -p enterprise_tracking enterprise_mlruns models artifacts logs data

# Set environment variables
ENV PYTHONPATH=/app
ENV MLFLOW_TRACKING_URI=./enterprise_mlruns

# Expose ports
EXPOSE 5000 8080

# Default command
CMD ["python", "main.py"]
"""
        
        with open(self.project_path / "Dockerfile", "w") as f:
            f.write(dockerfile_content)
    
    def _create_docker_compose(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á docker-compose.yml"""
        
        compose_content = f"""# Docker Compose for {self.project_path.name}
# Generated on {self.timestamp}

version: '3.8'

services:
  app:
    build: .
    ports:
      - "5000:5000"
      - "8080:8080"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./enterprise_tracking:/app/enterprise_tracking
      - ./enterprise_mlruns:/app/enterprise_mlruns
    environment:
      - PYTHONPATH=/app
      - MLFLOW_TRACKING_URI=./enterprise_mlruns
    restart: unless-stopped
    
  mlflow:
    image: python:{platform.python_version()}-slim
    ports:
      - "5001:5000"
    volumes:
      - ./enterprise_mlruns:/mlruns
    command: >
      sh -c "pip install mlflow && 
             mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root /mlruns"
    restart: unless-stopped

  jupyter:
    image: jupyter/datascience-notebook
    ports:
      - "8888:8888"
    volumes:
      - .:/home/jovyan/work
    environment:
      - JUPYTER_ENABLE_LAB=yes
    restart: unless-stopped
"""
        
        with open(self.project_path / "docker-compose.yml", "w") as f:
            f.write(compose_content)
    
    def _create_zip_package(self, output_path: str):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP package"""
        
        exclude_patterns = {
            "__pycache__", "*.pyc", "*.pyo", "*.pyd", ".git", ".vscode", 
            ".idea", "*.log", ".DS_Store", "Thumbs.db", "*.tmp",
            "node_modules", ".env", "*.env"
        }
        
        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(self.project_path):
                # Filter out excluded directories
                dirs[:] = [d for d in dirs if not any(pattern in d for pattern in exclude_patterns)]
                
                for file in files:
                    if not any(pattern in file for pattern in exclude_patterns):
                        file_path = Path(root) / file
                        arc_path = file_path.relative_to(self.project_path)
                        zipf.write(file_path, arc_path)
    
    def _create_migration_docs(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö migration"""
        
        readme_content = f"""# Project Migration Guide
# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå

Generated on: {self.timestamp}
Project: {self.project_path.name}

## üìã ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Package ‡∏ô‡∏µ‡πâ

### üóÇÔ∏è ‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå
- ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (.py, .yaml, .md, etc.)
- Configuration files
- Data files (‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å)
- Model files
- Documentation

### üîß Environment Files
- `requirements_freeze.txt` - Python packages (pip freeze)
- `requirements.txt` - Project requirements (pipreqs)
- `environment.yml` - Conda environment
- `Dockerfile` - Docker container
- `docker-compose.yml` - Multi-container setup

### üìä Analysis Files
- `migration_analysis.json` - Project analysis
- `environment_snapshot.json` - Environment details
- `MIGRATION_GUIDE.md` - This guide

## üöÄ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà

### Option 1: Python Virtual Environment
```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment
python -m venv project_env
source project_env/bin/activate  # Linux/Mac
# ‡∏´‡∏£‡∏∑‡∏≠
project_env\\Scripts\\activate     # Windows

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
pip install -r requirements_freeze.txt

# ‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå
python main.py
```

### Option 2: Conda Environment
```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡∏à‡∏≤‡∏Å file
conda env create -f environment.yml

# Activate environment
conda activate {self.project_path.name}_env

# ‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå
python main.py
```

### Option 3: Docker
```bash
# Build ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô container
docker-compose up --build

# ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ô Docker ‡πÅ‡∏¢‡∏Å
docker build -t {self.project_path.name.lower()} .
docker run -p 5000:5000 -p 8080:8080 {self.project_path.name.lower()}
```

### Option 4: Manual Setup
```bash
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python dependencies
pip install mlflow rich pandas numpy scikit-learn click typer pyyaml

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
mkdir -p enterprise_tracking enterprise_mlruns models artifacts logs data

# Copy configuration files
cp tracking_config.yaml [destination]
cp *.py [destination]

# ‡∏£‡∏±‡∏ô initialization
python init_tracking_system.py
```

## üîß Configuration Setup

### 1. ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á tracking_config.yaml
- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô paths ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà
- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ database/cloud storage ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
- Enable/disable features ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

### 2. Environment Variables
```bash
export MLFLOW_TRACKING_URI=./enterprise_mlruns
export PYTHONPATH=/path/to/project
```

### 3. Port Configuration
- MLflow UI: http://localhost:5000
- Application: http://localhost:8080
- Jupyter: http://localhost:8888

## üìÇ Directory Structure
```
project/
‚îú‚îÄ‚îÄ tracking.py              # Main tracking system
‚îú‚îÄ‚îÄ tracking_config.yaml     # Configuration
‚îú‚îÄ‚îÄ tracking_cli.py          # CLI interface
‚îú‚îÄ‚îÄ tracking_examples.py     # Usage examples
‚îú‚îÄ‚îÄ ProjectP.py              # Main application
‚îú‚îÄ‚îÄ enterprise_tracking/     # Local tracking data
‚îú‚îÄ‚îÄ enterprise_mlruns/       # MLflow data
‚îú‚îÄ‚îÄ models/                  # Saved models
‚îú‚îÄ‚îÄ artifacts/               # Experiment artifacts
‚îú‚îÄ‚îÄ logs/                    # Application logs
‚îî‚îÄ‚îÄ data/                    # Data files
```

## üîç Verification Steps

### 1. Test Basic Setup
```bash
python -c "import tracking; print('Tracking system OK')"
python tracking_examples.py
```

### 2. Test MLflow
```bash
mlflow ui --backend-store-uri ./enterprise_mlruns
```

### 3. Test CLI
```bash
python tracking_cli.py list-experiments
python tracking_cli.py system-status
```

## üêõ Troubleshooting

### Common Issues:
1. **Import Errors**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PYTHONPATH ‡πÅ‡∏•‡∏∞ virtual environment
2. **Permission Errors**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö file permissions
3. **Port Conflicts**: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô port ‡πÉ‡∏ô configuration
4. **Database Issues**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö database connection string

### Solutions:
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Python environment
python --version
pip list

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö file permissions
ls -la

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö port availability
netstat -an | grep :5000
```

## üìû Support
‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs ‡πÉ‡∏ô `logs/tracking.log`
2. ‡∏£‡∏±‡∏ô `python tracking_cli.py system-status`
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö `migration_analysis.json` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå

## üìù Notes
- Package ‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏° environment snapshot ‡∏ì ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {self.timestamp}
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö compatibility ‡∏Å‡∏±‡∏ö Python version ‡πÅ‡∏•‡∏∞ OS ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
- Data files ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô package
- Model files ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏¢‡∏Å‡∏´‡∏≤‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

Happy coding! üöÄ
"""
        
        with open(self.project_path / "MIGRATION_GUIDE.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
    
    def _get_file_structure(self) -> Dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        structure = {}
        for root, dirs, files in os.walk(self.project_path):
            rel_root = Path(root).relative_to(self.project_path)
            structure[str(rel_root)] = {
                "directories": dirs,
                "files": files,
                "file_count": len(files),
                "dir_count": len(dirs)
            }
        return structure
    
    def _analyze_dependencies(self) -> Dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå dependencies"""
        deps = {
            "requirements_files": [],
            "import_statements": [],
            "conda_deps": [],
            "system_deps": []
        }
        
        # Find requirements files
        for req_file in ["requirements.txt", "requirements-dev.txt", "requirements_freeze.txt"]:
            if (self.project_path / req_file).exists():
                deps["requirements_files"].append(req_file)
        
        # Find import statements
        for py_file in self.project_path.rglob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    for line in content.split('\n'):
                        line = line.strip()
                        if line.startswith(('import ', 'from ')):
                            deps["import_statements"].append(line)
            except:
                pass
        
        return deps
    
    def _find_config_files(self) -> List[str]:
        """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå configuration"""
        config_patterns = ["*.yaml", "*.yml", "*.json", "*.toml", "*.ini", "*.cfg", "*.env"]
        config_files = []
        
        for pattern in config_patterns:
            config_files.extend([str(f.relative_to(self.project_path)) 
                               for f in self.project_path.rglob(pattern)])
        
        return config_files
    
    def _find_data_files(self) -> List[str]:
        """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        data_patterns = ["*.csv", "*.json", "*.parquet", "*.xlsx", "*.pickle", "*.pkl"]
        data_files = []
        
        for pattern in data_patterns:
            data_files.extend([str(f.relative_to(self.project_path)) 
                             for f in self.project_path.rglob(pattern)])
        
        return data_files
    
    def _find_model_files(self) -> List[str]:
        """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        model_patterns = ["*.pkl", "*.joblib", "*.h5", "*.pb", "*.onnx", "*.pth", "*.pt"]
        model_files = []
        
        for pattern in model_patterns:
            model_files.extend([str(f.relative_to(self.project_path)) 
                              for f in self.project_path.rglob(pattern)])
        
        return model_files
    
    def _analyze_tracking_data(self) -> Dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• tracking"""
        tracking_info = {}
        
        # MLflow data
        mlflow_dir = self.project_path / "enterprise_mlruns"
        if mlflow_dir.exists():
            tracking_info["mlflow"] = {
                "exists": True,
                "experiments": len(list(mlflow_dir.glob("*"))),
                "size_mb": self._get_directory_size(mlflow_dir) / (1024 * 1024)
            }
        
        # Local tracking data
        tracking_dir = self.project_path / "enterprise_tracking"
        if tracking_dir.exists():
            tracking_info["local"] = {
                "exists": True,
                "runs": len(list(tracking_dir.glob("*"))),
                "size_mb": self._get_directory_size(tracking_dir) / (1024 * 1024)
            }
        
        return tracking_info
    
    def _get_directory_size(self, path: Path) -> int:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏î‡πÄ‡∏£‡πá‡∏Å‡∏ó‡∏≠‡∏£‡∏µ"""
        total_size = 0
        try:
            for dirpath, dirnames, filenames in os.walk(path):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total_size += os.path.getsize(filepath)
        except:
            pass
        return total_size
    
    def _get_system_info(self) -> Dict:
        """‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö"""
        return {
            "platform": platform.platform(),
            "system": platform.system(),
            "release": platform.release(),
            "version": platform.version(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "python_implementation": platform.python_implementation()
        }
    
    def _get_python_info(self) -> Dict:
        """‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Python"""
        import sys
        return {
            "version": sys.version,
            "version_info": list(sys.version_info),
            "executable": sys.executable,
            "path": sys.path[:5]  # First 5 paths only
        }
    
    def _get_installed_packages(self) -> List[str]:
        """‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ packages ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á"""
        try:
            result = subprocess.run(["pip", "list", "--format=freeze"], 
                                  capture_output=True, text=True)
            return result.stdout.strip().split('\n')
        except:
            return []
    
    def _get_relevant_env_vars(self) -> Dict:
        """Environment variables ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"""
        relevant_vars = [
            "PYTHONPATH", "PATH", "MLFLOW_TRACKING_URI", "WANDB_API_KEY",
            "CUDA_VISIBLE_DEVICES", "TF_CPP_MIN_LOG_LEVEL"
        ]
        
        env_vars = {}
        for var in relevant_vars:
            if var in os.environ:
                env_vars[var] = os.environ[var]
        
        return env_vars
    
    def _get_conda_info(self) -> Dict:
        """‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Conda"""
        try:
            result = subprocess.run(["conda", "info", "--json"], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                return json.loads(result.stdout)
        except:
            pass
        return {"available": False}
    
    def _get_git_info(self) -> Dict:
        """‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Git"""
        git_info = {"available": False}
        
        try:
            # Check if git repo
            result = subprocess.run(["git", "rev-parse", "--git-dir"], 
                                  capture_output=True, text=True, cwd=self.project_path)
            if result.returncode == 0:
                git_info["available"] = True
                
                # Get current branch
                result = subprocess.run(["git", "branch", "--show-current"], 
                                      capture_output=True, text=True, cwd=self.project_path)
                git_info["branch"] = result.stdout.strip()
                
                # Get last commit
                result = subprocess.run(["git", "log", "-1", "--format=%H %s"], 
                                      capture_output=True, text=True, cwd=self.project_path)
                git_info["last_commit"] = result.stdout.strip()
                
                # Get remote URL
                result = subprocess.run(["git", "remote", "get-url", "origin"], 
                                      capture_output=True, text=True, cwd=self.project_path)
                git_info["remote_url"] = result.stdout.strip()
                
        except:
            pass
        
        return git_info


class ProjectDeployment:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö deploy ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà"""
    
    def __init__(self, migration_package: str):
        self.package_path = Path(migration_package)
        self.extract_path = None
        
    def extract_and_setup(self, destination: str = "./extracted_project") -> str:
        """‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå"""
        
        self.extract_path = Path(destination)
        self.extract_path.mkdir(exist_ok=True)
        
        console.print(f"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå {self.package_path}...", style="bold blue")
        
        with zipfile.ZipFile(self.package_path, 'r') as zipf:
            zipf.extractall(self.extract_path)
        
        console.print(f"‚úÖ ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à: {self.extract_path}", style="bold green")
        
        # Load migration data
        analysis_file = self.extract_path / "migration_analysis.json"
        if analysis_file.exists():
            with open(analysis_file, 'r', encoding='utf-8') as f:
                self.migration_data = json.load(f)
        
        return str(self.extract_path)
    
    def setup_environment(self, method: str = "pip"):
        """‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°"""
        
        if not self.extract_path:
            raise ValueError("‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô")
        
        console.print(f"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ {method}...", style="bold blue")
        
        os.chdir(self.extract_path)
        
        if method == "pip":
            self._setup_pip_environment()
        elif method == "conda":
            self._setup_conda_environment()
        elif method == "docker":
            self._setup_docker_environment()
        
        console.print("‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à", style="bold green")
    
    def _setup_pip_environment(self):
        """‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ pip"""
        requirements_files = ["requirements_freeze.txt", "requirements.txt"]
        
        for req_file in requirements_files:
            if Path(req_file).exists():
                console.print(f"üì¶ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å {req_file}...")
                subprocess.run(["pip", "install", "-r", req_file], check=True)
                break
    
    def _setup_conda_environment(self):
        """‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ conda"""
        if Path("environment.yml").exists():
            console.print("üì¶ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å environment.yml...")
            subprocess.run(["conda", "env", "create", "-f", "environment.yml"], check=True)
    
    def _setup_docker_environment(self):
        """‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ Docker"""
        if Path("docker-compose.yml").exists():
            console.print("üê≥ ‡∏£‡∏±‡∏ô Docker Compose...")
            subprocess.run(["docker-compose", "up", "--build", "-d"], check=True)
        elif Path("Dockerfile").exists():
            console.print("üê≥ Build Docker Image...")
            subprocess.run(["docker", "build", "-t", "migrated-project", "."], check=True)


def main():
    """‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å"""
    console.print(Panel("üöÄ Project Migration System", style="bold blue"))
    
    action = Prompt.ask(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
        choices=["migrate", "deploy", "help"],
        default="migrate"
    )
    
    if action == "migrate":
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á migration package
        project_path = Prompt.ask("Project path", default=".")
        output_path = Prompt.ask("Output package name", default=None)
        
        migrator = ProjectMigrator(project_path)
        package_path = migrator.create_migration_package(output_path)
        
        console.print(f"üéâ ‡∏™‡∏£‡πâ‡∏≤‡∏á Migration Package ‡πÄ‡∏™‡∏£‡πá‡∏à: {package_path}", style="bold green")
        console.print("üìñ ‡∏≠‡πà‡∏≤‡∏ô MIGRATION_GUIDE.md ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á", style="bold yellow")
        
    elif action == "deploy":
        # Deploy ‡∏à‡∏≤‡∏Å migration package
        package_path = Prompt.ask("Migration package path")
        destination = Prompt.ask("Destination directory", default="./extracted_project")
        method = Prompt.ask("Setup method", choices=["pip", "conda", "docker"], default="pip")
        
        deployer = ProjectDeployment(package_path)
        extract_path = deployer.extract_and_setup(destination)
        deployer.setup_environment(method)
        
        console.print(f"üéâ Deploy ‡πÄ‡∏™‡∏£‡πá‡∏à: {extract_path}", style="bold green")
        
    elif action == "help":
        console.print(Panel("""
üîß ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Project Migration System

1. **‡∏™‡∏£‡πâ‡∏≤‡∏á Migration Package** (migrate):
   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP package ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
   - ‡∏£‡∏ß‡∏° Dockerfile, requirements, environment.yml

2. **Deploy Project** (deploy):
   - ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå migration package
   - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà
   - ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ

3. **‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:
   - MIGRATION_GUIDE.md - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
   - migration_analysis.json - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå
   - environment_snapshot.json - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
        """, title="Help", style="bold yellow"))


if __name__ == "__main__":
    main()
