#!/usr/bin/env python3
"""
üî• INTEGRATED EMERGENCY FIXES MODULE
‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£ emergency fix logic ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á pipeline
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
from datetime import datetime
import traceback
from typing import Optional, Dict, Any, Tuple, List
import yaml
import os

warnings.filterwarnings('ignore')

class EmergencyFixManager:
    """Manager ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ emergency ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    
    def __init__(self, output_dir: str = "output_default"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.log_file = self.output_dir / "emergency_fixes.log"
        
    def log_fix(self, message: str, level: str = "INFO"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}\n"
        
        with open(self.log_file, "a", encoding='utf-8') as f:
            f.write(log_message)
        
        print(f"üîß [{level}] {message}")
    
    def check_data_health(self, df: pd.DataFrame, target_col: str = 'target') -> Tuple[bool, List[str]]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏ç‡∏´‡∏≤"""
        issues = []
        
        try:
            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö target column
            if target_col not in df.columns:
                issues.append(f"Missing target column: {target_col}")
                return False, issues
            
            # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö class distribution
            target_counts = df[target_col].value_counts()
            if len(target_counts) < 2:
                issues.append("Target has only one class")
            else:
                class_ratio = target_counts.max() / target_counts.min()
                if class_ratio > 100:
                    issues.append(f"Extreme class imbalance: {class_ratio:.1f}:1")
            
            # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NaN values
            nan_percentage = df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100
            if nan_percentage > 10:
                issues.append(f"High NaN percentage: {nan_percentage:.1f}%")
            
            # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö feature correlation
            feature_cols = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
            if len(feature_cols) > 1:
                corr_matrix = df[feature_cols + [target_col]].corr()
                max_target_corr = abs(corr_matrix[target_col][feature_cols]).max()
                if max_target_corr < 0.05:
                    issues.append(f"Very low feature-target correlation: {max_target_corr:.4f}")
            
            # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data size
            if len(df) < 100:
                issues.append(f"Insufficient data size: {len(df)} rows")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏£‡πâ‡∏≤‡∏¢ ‡πÅ‡∏£‡∏á
            critical_keywords = ["Missing target", "only one class", "Extreme class imbalance", "Very low"]
            has_critical_issues = any(any(keyword in issue for keyword in critical_keywords) for issue in issues)
            
            return not has_critical_issues, issues
            
        except Exception as e:
            issues.append(f"Health check error: {str(e)}")
            return False, issues
    
    def auto_fix_data(self, df: pd.DataFrame, target_col: str = 'target') -> pd.DataFrame:
        """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
        self.log_fix("Starting automatic data fixing...", "INFO")
        
        df_fixed = df.copy()
        
        try:
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á target column ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
            if target_col not in df_fixed.columns:
                self.log_fix(f"Creating missing target column: {target_col}", "WARN")
                if len(df_fixed.columns) > 0:
                    # ‡πÉ‡∏ä‡πâ median split ‡∏à‡∏≤‡∏Å column ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                    numeric_cols = df_fixed.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        base_col = numeric_cols[0]
                        median_val = df_fixed[base_col].median()
                        df_fixed[target_col] = (df_fixed[base_col] > median_val).astype(int)
                        self.log_fix(f"Created target from {base_col} median split", "INFO")
                    else:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á random target
                        np.random.seed(42)
                        df_fixed[target_col] = np.random.choice([0, 1], size=len(df_fixed), p=[0.7, 0.3])
                        self.log_fix("Created random target column", "WARN")
            
            # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î features
            feature_cols = [col for col in df_fixed.columns if col != target_col]
            
            for col in feature_cols:
                if df_fixed[col].dtype in ['int64', 'float64']:
                    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡πÅ‡∏•‡∏∞ infinite values
                    df_fixed[col] = df_fixed[col].fillna(df_fixed[col].median())
                    df_fixed[col] = df_fixed[col].replace([np.inf, -np.inf], df_fixed[col].median())
            
            self.log_fix("Cleaned NaN and infinite values", "INFO")
            
            # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á engineered features
            numeric_cols = [col for col in feature_cols if df_fixed[col].dtype in ['int64', 'float64']]
            
            if len(numeric_cols) >= 2:
                # Moving averages
                for col in numeric_cols[:3]:
                    df_fixed[f'{col}_ma3'] = df_fixed[col].rolling(3, min_periods=1).mean()
                
                # Interaction features
                df_fixed['feature_sum'] = df_fixed[numeric_cols[0]] + df_fixed[numeric_cols[1]]
                df_fixed['feature_ratio'] = df_fixed[numeric_cols[0]] / (df_fixed[numeric_cols[1]] + 1e-8)
                
                # Rank features
                for col in numeric_cols[:2]:
                    df_fixed[f'{col}_rank'] = df_fixed[col].rank(pct=True)
                
                self.log_fix("Created engineered features", "INFO")
            
            # 4. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç class imbalance
            target_counts = df_fixed[target_col].value_counts()
            if len(target_counts) >= 2:
                class_ratio = target_counts.max() / target_counts.min()
                if class_ratio > 50:  # Extreme imbalance
                    self.log_fix(f"Fixing extreme class imbalance: {class_ratio:.1f}:1", "WARN")
                    df_fixed = self._balance_classes(df_fixed, target_col)
            
            # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            is_healthy, remaining_issues = self.check_data_health(df_fixed, target_col)
            if is_healthy:
                self.log_fix("Data successfully fixed!", "SUCCESS")
            else:
                self.log_fix(f"Some issues remain: {remaining_issues}", "WARN")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
            output_file = self.output_dir / "fixed_data.csv"
            df_fixed.to_csv(output_file, index=False)
            self.log_fix(f"Saved fixed data to {output_file}", "INFO")
            
            return df_fixed
            
        except Exception as e:
            self.log_fix(f"Error in auto_fix_data: {str(e)}", "ERROR")
            self.log_fix(f"Traceback: {traceback.format_exc()}", "DEBUG")
            return df_fixed
    
    def _balance_classes(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏• classes"""
        try:
            target_counts = df[target_col].value_counts()
            min_samples = max(100, target_counts.min() * 3)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å minority class
            
            balanced_dfs = []
            for class_val in target_counts.index:
                class_df = df[df[target_col] == class_val]
                
                if len(class_df) < min_samples:
                    # Oversample
                    additional_samples = min_samples - len(class_df)
                    resampled = class_df.sample(n=additional_samples, replace=True, random_state=42)
                    balanced_df = pd.concat([class_df, resampled], ignore_index=True)
                elif len(class_df) > min_samples * 3:
                    # Undersample
                    balanced_df = class_df.sample(n=min_samples * 2, random_state=42)
                else:
                    balanced_df = class_df
                
                balanced_dfs.append(balanced_df)
            
            result_df = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=42)
            self.log_fix(f"Balanced classes: {result_df[target_col].value_counts().to_dict()}", "INFO")
            
            return result_df
            
        except Exception as e:
            self.log_fix(f"Error in _balance_classes: {str(e)}", "ERROR")
            return df
    
    def prepare_model_safe_data(self, df: pd.DataFrame, target_col: str = 'target') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö model training"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            is_healthy, issues = self.check_data_health(df, target_col)
            
            if not is_healthy:
                self.log_fix(f"Data issues detected: {issues}", "WARN")
                df = self.auto_fix_data(df, target_col)
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features ‡πÅ‡∏•‡∏∞ target
            feature_cols = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
            
            X = df[feature_cols].copy()
            y = df[target_col].copy()
            
            # Standardize features
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_scaled = pd.DataFrame(
                scaler.fit_transform(X),
                columns=X.columns,
                index=X.index
            )
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata
            metadata = {
                'feature_columns': feature_cols,
                'target_column': target_col,
                'original_shape': df.shape,
                'processed_shape': (len(X_scaled), len(feature_cols)),
                'class_distribution': y.value_counts().to_dict(),
                'scaler_params': {
                    'mean': scaler.mean_.tolist(),
                    'scale': scaler.scale_.tolist()
                }
            }
            
            # ‡∏£‡∏ß‡∏° X ‡πÅ‡∏•‡∏∞ y ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
            final_df = X_scaled.copy()
            final_df[target_col] = y.values
            
            self.log_fix(f"Prepared safe data: {final_df.shape}", "SUCCESS")
            
            return final_df, metadata
            
        except Exception as e:
            self.log_fix(f"Error in prepare_model_safe_data: {str(e)}", "ERROR")
            # Return original data if processing fails
            return df, {'error': str(e)}

    def validate_and_fix_config(self, config_path: str = "config.yaml") -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç config.yaml ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"""
        self.log_fix("Starting config validation and auto-fix...")
        
        try:
            config_file = Path(config_path)
            
            # Template config ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            complete_config = {
                'data': {
                    'source': 'dummy_m1.csv',
                    'target_column': 'target',
                    'features': ['Open', 'High', 'Low', 'Close', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd']
                },
                'model_class': 'RandomForestClassifier',
                'model_params': {
                    'n_estimators': 100,
                    'max_depth': 10,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'class_weight': 'balanced',
                    'random_state': 42,
                    'n_jobs': -1
                },
                'model': {
                    'type': 'RandomForest',
                    'file': 'models/rf_model.joblib',
                    'features': ['Open', 'High', 'Low', 'Close', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd']
                },
                'training': {
                    'test_size': 0.3,
                    'random_state': 42,
                    'cross_validation': 5
                },
                'walk_forward': {
                    'enabled': True,
                    'window_size': 1000,
                    'step_size': 100,
                    'min_train_size': 500
                },                'metrics': ['auc', 'accuracy', 'precision', 'recall', 'f1_score', 'balanced_accuracy', 'roc_auc'],
                'export': {
                    'output_dir': 'output_default',
                    'save_features': True,
                    'save_model': True,
                    'save_predictions': True,
                    'save_reports': True,
                    'save_logs': True
                },
                'parallel': {
                    'enabled': True,
                    'n_jobs': -1,
                    'backend': 'threading'
                },
                'visualization': {
                    'enabled': True,
                    'show_plots': False,
                    'save_plots': True,
                    'plot_dir': 'output_default/plots'
                },
                'emergency_fixes': {
                    'enabled': True,
                    'auto_fix_data': True,
                    'create_missing_target': True,
                    'balance_classes': True,
                    'handle_nan_features': True
                }
            }
            
            # ‡πÇ‡∏´‡∏•‡∏î config ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        current_config = yaml.safe_load(f) or {}
                    self.log_fix(f"Loaded existing config: {len(current_config)} sections")
                except Exception as e:
                    self.log_fix(f"Error loading config: {e}", "WARN")
                    current_config = {}
            else:
                self.log_fix("Config file not found, creating new one")
                current_config = {}
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
            missing_fields = []
            updated_config = current_config.copy()
            
            def check_and_add_field(config_dict, key, default_value, path=""):
                full_path = f"{path}.{key}" if path else key
                
                if key not in config_dict:
                    config_dict[key] = default_value
                    missing_fields.append(full_path)
                    return True
                elif isinstance(default_value, dict) and isinstance(config_dict[key], dict):
                    # Recursive check for nested dicts
                    changed = False
                    for sub_key, sub_value in default_value.items():
                        if check_and_add_field(config_dict[key], sub_key, sub_value, full_path):
                            changed = True
                    return changed
                return False
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå
            config_changed = False
            for key, value in complete_config.items():
                if check_and_add_field(updated_config, key, value):
                    config_changed = True
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å config ‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
            if config_changed or missing_fields:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á backup ‡∏Ç‡∏≠‡∏á config ‡πÄ‡∏Å‡πà‡∏≤
                if config_file.exists():
                    backup_file = config_file.with_suffix('.yaml.backup')
                    import shutil
                    shutil.copy2(config_file, backup_file)
                    self.log_fix(f"Created backup: {backup_file}")
                
                # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô config ‡πÉ‡∏´‡∏°‡πà
                with open(config_file, 'w', encoding='utf-8') as f:
                    yaml.dump(updated_config, f, default_flow_style=False, indent=2, allow_unicode=True)
                
                self.log_fix(f"Updated config with {len(missing_fields)} missing fields:")
                for field in missing_fields[:10]:  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 10 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
                    self.log_fix(f"  + Added: {field}")
                
                if len(missing_fields) > 10:
                    self.log_fix(f"  + ... and {len(missing_fields) - 10} more fields")
            
            self.log_fix("Config validation completed successfully")
            return True
            
        except Exception as e:
            self.log_fix(f"Config validation failed: {e}", "ERROR")
            traceback.print_exc()
            return False

def ensure_complete_config():
    """Ensure config.yaml has all required fields for pipeline execution"""
    config_path = Path("config.yaml")
    
    # Default complete config template
    default_config = {
        'data': {
            'source': 'dummy_m1.csv',
            'target_column': 'target'
        },
        'model': {
            'features': ['Open', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd'],
            'file': 'models/rf_model.joblib',
            'type': 'RandomForest',
            'model_class': 'RandomForestClassifier',
            'model_params': {
                'n_estimators': 100,
                'max_depth': 10,
                'random_state': 42,
                'n_jobs': -1
            }
        },
        'training': {
            'cross_validation': 5,
            'random_state': 42,
            'test_size': 0.3,
            'walk_forward': {
                'enabled': True,
                'window_size': 1000,
                'step_size': 100
            }
        },
        'metrics': {
            'primary': 'auc',
            'additional': ['accuracy', 'precision', 'recall', 'f1_score']
        },
        'export': {
            'output_dir': 'output_default',
            'save_model': True,
            'save_predictions': True,
            'save_features': True,
            'save_reports': True
        },
        'parallel': {
            'enabled': True,
            'n_jobs': -1,
            'backend': 'threading'
        },
        'visualization': {
            'enabled': True,
            'save_plots': True,
            'show_plots': False,
            'plot_dir': 'output_default/plots'
        }
    }
    
    try:
        # Load existing config
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f) or {}
        else:
            config = {}
        
        # Merge with defaults to fill missing fields
        def merge_configs(base, update):
            for key, value in update.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    merge_configs(base[key], value)
                else:
                    base[key] = value
        
        complete_config = default_config.copy()
        merge_configs(complete_config, config)
        
        # Write complete config back
        with open(config_path, 'w') as f:
            yaml.dump(complete_config, f, default_flow_style=False, indent=2)
        
        print(f"‚úÖ Config validated and updated: {config_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå Config repair failed: {e}")
        # Create fresh config if repair fails
        try:
            with open(config_path, 'w') as f:
                yaml.dump(default_config, f, default_flow_style=False, indent=2)
            print(f"‚úÖ Fresh config created: {config_path}")
            return True
        except Exception as e2:
            print(f"‚ùå Fresh config creation failed: {e2}")
            return False

# Factory function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á emergency fix manager
def create_emergency_fix_manager(output_dir: str = "output_default") -> EmergencyFixManager:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á emergency fix manager"""
    return EmergencyFixManager(output_dir)

# Integration functions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô pipeline modes
def apply_emergency_fixes_to_pipeline(mode: str = "full_pipeline", **kwargs) -> bool:
    """‡πÉ‡∏ä‡πâ emergency fixes ‡∏Å‡∏±‡∏ö pipeline mode ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏"""
    fix_manager = create_emergency_fix_manager()
    
    try:
        fix_manager.log_fix(f"Starting emergency fixes for mode: {mode}", "INFO")
        
        # Step 1: Validate and fix config first
        if not fix_manager.validate_and_fix_config():
            fix_manager.log_fix("Config validation failed", "ERROR")
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        data_files = [
            "dummy_m1.csv", "dummy_m15.csv",
            "data/dummy_m1.csv", "data/dummy_m15.csv",
            "output_default/processed_data.csv"
        ]
        
        df = None
        for file_path in data_files:
            if Path(file_path).exists():
                try:
                    df = pd.read_csv(file_path)
                    fix_manager.log_fix(f"Loaded data from {file_path}: {df.shape}", "INFO")
                    break
                except Exception as e:
                    fix_manager.log_fix(f"Failed to load {file_path}: {e}", "WARN")
        
        if df is None:
            fix_manager.log_fix("No data found, creating synthetic data", "WARN")
            df = _create_synthetic_data(fix_manager)
        
        # ‡πÉ‡∏ä‡πâ emergency fixes
        fixed_df, metadata = fix_manager.prepare_model_safe_data(df)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        output_file = fix_manager.output_dir / f"emergency_fixed_{mode}.csv"
        fixed_df.to_csv(output_file, index=False)
        
        metadata_file = fix_manager.output_dir / f"emergency_metadata_{mode}.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        fix_manager.log_fix(f"Emergency fixes completed for {mode}", "SUCCESS")
        return True
        
    except Exception as e:
        fix_manager.log_fix(f"Emergency fixes failed for {mode}: {str(e)}", "ERROR")
        return False

def _create_synthetic_data(fix_manager: EmergencyFixManager) -> pd.DataFrame:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• synthetic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö testing"""
    try:
        np.random.seed(42)
        n_samples = 5000
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á features
        features = {}
        feature_names = ['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd']
        
        for i, name in enumerate(feature_names):
            base_signal = np.random.randn(n_samples) * 0.1
            if i > 0:  # correlation
                base_signal += features[feature_names[0]] * 0.05
            features[name] = base_signal
        
        df = pd.DataFrame(features)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á target with moderate imbalance
        target_probs = np.random.random(n_samples)
        df['target'] = np.where(target_probs < 0.2, 1, 0)  # 20% class 1
        
        fix_manager.log_fix(f"Created synthetic data: {df.shape}", "INFO")
        fix_manager.log_fix(f"Target distribution: {df['target'].value_counts().to_dict()}", "INFO")
        
        return df
        
    except Exception as e:
        fix_manager.log_fix(f"Error creating synthetic data: {str(e)}", "ERROR")
        # Return minimal data
        return pd.DataFrame({
            'feature1': [1, 2, 3, 4, 5],
            'feature2': [2, 3, 4, 5, 6],
            'target': [0, 1, 0, 1, 0]
        })

# Main testing function
if __name__ == "__main__":
    print("üî• Testing Integrated Emergency Fixes...")
    
    # Ensure config is complete
    ensure_complete_config()
    
    # Test for different modes
    modes = ["full_pipeline", "debug_full_pipeline", "preprocess", "realistic_backtest"]
    
    for mode in modes:
        print(f"\nüß™ Testing {mode}...")
        success = apply_emergency_fixes_to_pipeline(mode)
        
        if success:
            print(f"‚úÖ {mode} emergency fixes completed")
        else:
            print(f"‚ùå {mode} emergency fixes failed")
    
    print("\nüéâ Integration testing completed!")
