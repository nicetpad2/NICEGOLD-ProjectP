#!/usr/bin/env python3
from datetime import datetime
from pathlib import Path
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import classification_report, roc_auc_score
        from sklearn.model_selection import cross_val_score, StratifiedKFold
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
import json
import numpy as np
import pandas as pd
        import traceback
import warnings
"""
üö® EMERGENCY NaN AUC FIX
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ AUC = nan ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å:
1. Class imbalance ‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á (201:1)
2. Features correlation ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
3. Data quality issues
4. Model convergence problems
"""


warnings.filterwarnings('ignore')

def create_output_dir():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á output directory"""
    output_dir = Path("output_default")
    output_dir.mkdir(exist_ok = True)
    return output_dir

def load_test_data():
    """‡πÇ‡∏´‡∏•‡∏î data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö test"""
    print("üîç Loading test data...")

    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ data files
    data_files = [
        "dummy_m1.csv", "dummy_m15.csv", 
        "data/dummy_m1.csv", "data/dummy_m15.csv"
    ]

    df = None
    for file_path in data_files:
        if Path(file_path).exists():
            try:
                df = pd.read_csv(file_path)
                print(f"‚úÖ Loaded data from {file_path}: {df.shape}")
                print(f"üìã Columns found: {list(df.columns)}")

                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ target column ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if 'target' not in df.columns:
                    print(f"‚ö†Ô∏è No 'target' column found in {file_path}")
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ target ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤
                    if len(df.columns) > 0:
                        # ‡πÉ‡∏ä‡πâ column ‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á target
                        base_col = df.columns[0]
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á target ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤ median
                        median_val = df[base_col].median()
                        df['target'] = (df[base_col] > median_val).astype(int)
                        print(f"‚úÖ Created target column from {base_col} (median split)")
                        print(f"üìä Target distribution: {df['target'].value_counts().to_dict()}")
                    else:
                        print(f"‚ùå No usable columns in {file_path}")
                        continue

                break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {file_path}: {e}")

    if df is None or 'target' not in df.columns:
        print("üîß Creating synthetic data for testing...")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic data ‡∏ó‡∏µ‡πà‡∏°‡∏µ class imbalance ‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á
        np.random.seed(42)
        n_samples = 10000

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á features
        features = {}
        feature_names = ['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'momentum', 'volatility']

        for i, name in enumerate(feature_names):
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÅ‡∏•‡∏∞ correlation
            base_signal = np.random.randn(n_samples) * 0.1
            if i > 0:  # correlation with previous features
                base_signal += features[feature_names[0]] * 0.05
            features[name] = base_signal

        df = pd.DataFrame(features)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á target ‡∏ó‡∏µ‡πà‡∏°‡∏µ extreme imbalance
        # 99.5% class 0, 0.3% class 1, 0.2% class -1
        target_probs = np.random.random(n_samples)
        df['target'] = np.where(target_probs < 0.002, -1,  # 0.2% class -1
                       np.where(target_probs < 0.005, 1, 0))  # 0.3% class 1, rest class 0

        print(f"‚úÖ Created synthetic data: {df.shape}")
        print(f"üìä Synthetic target distribution: {df['target'].value_counts().to_dict()}")

    return df

def diagnose_nan_auc_causes(df, target_col = 'target'):
    """‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Ç‡∏≠‡∏á NaN AUC"""
    print("\nüîç Diagnosing NaN AUC causes...")

    issues = []

    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö target distribution
    target_counts = df[target_col].value_counts()
    print(f"üìä Target distribution: {target_counts.to_dict()}")

    if len(target_counts) < 2:
        issues.append("CRITICAL: Only one class in target")

    min_class_count = target_counts.min()
    max_class_count = target_counts.max()
    imbalance_ratio = max_class_count / min_class_count if min_class_count > 0 else float('inf')

    print(f"‚öñÔ∏è Imbalance ratio: {imbalance_ratio:.1f}:1")

    if imbalance_ratio > 100:
        issues.append(f"CRITICAL: Extreme class imbalance {imbalance_ratio:.1f}:1")

    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data quality
    feature_cols = [col for col in df.columns if col != target_col]

    nan_counts = df[feature_cols].isnull().sum()
    if nan_counts.sum() > 0:
        issues.append(f"DATA: {nan_counts.sum()} NaN values in features")

    inf_counts = np.isinf(df[feature_cols]).sum().sum()
    if inf_counts > 0:
        issues.append(f"DATA: {inf_counts} infinite values in features")

    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö feature variance
    zero_var_features = []
    for col in feature_cols:
        if df[col].var() == 0 or df[col].std() == 0:
            zero_var_features.append(col)

    if zero_var_features:
        issues.append(f"FEATURES: Zero variance features: {zero_var_features}")

    # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö feature - target correlation
    correlations = {}
    for col in feature_cols:
        try:
            corr = df[col].corr(df[target_col])
            if not np.isnan(corr):
                correlations[col] = abs(corr)
        except:
            pass

    max_corr = max(correlations.values()) if correlations else 0
    print(f"üìà Max feature - target correlation: {max_corr:.4f}")

    if max_corr < 0.01:
        issues.append(f"FEATURES: Very weak correlations (max: {max_corr:.4f})")

    return issues

def apply_aggressive_fixes(df, target_col = 'target'):
    """‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NaN AUC"""
    print("\nüõ†Ô∏è Applying aggressive fixes for NaN AUC...")

    df_fixed = df.copy()

    # 1. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î data
    feature_cols = [col for col in df_fixed.columns if col != target_col]

    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡πÅ‡∏•‡∏∞ infinite values
    for col in feature_cols:
        df_fixed[col] = df_fixed[col].fillna(df_fixed[col].median())
        df_fixed[col] = df_fixed[col].replace([np.inf, -np.inf], df_fixed[col].median())

    print("‚úÖ Cleaned NaN and infinite values")

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á engineered features ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á
    print("üîß Creating robust engineered features...")

    # Moving averages
    for col in feature_cols[:3]:  # first 3 features only to avoid too many
        if df_fixed[col].dtype in ['int64', 'float64']:
            df_fixed[f'{col}_ma3'] = df_fixed[col].rolling(3, min_periods = 1).mean()
            df_fixed[f'{col}_std3'] = df_fixed[col].rolling(3, min_periods = 1).std().fillna(0)

    # Interaction features
    if len(feature_cols) >= 2:
        df_fixed['feature_sum'] = df_fixed[feature_cols[0]] + df_fixed[feature_cols[1]]
        df_fixed['feature_ratio'] = df_fixed[feature_cols[0]] / (df_fixed[feature_cols[1]] + 1e - 8)

    # Percentile features
    for col in feature_cols[:2]:
        if df_fixed[col].dtype in ['int64', 'float64']:
            df_fixed[f'{col}_rank'] = df_fixed[col].rank(pct = True)

    print("‚úÖ Created engineered features")

    # 3. ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á target distribution ‡∏î‡πâ‡∏ß‡∏¢ synthetic sampling
    print("‚öñÔ∏è Balancing target classes...")

    target_counts = df_fixed[target_col].value_counts()
    min_samples = max(50, target_counts.min())  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 50 samples per class

    balanced_dfs = []
    for class_val in target_counts.index:
        class_df = df_fixed[df_fixed[target_col] == class_val]

        if len(class_df) < min_samples:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic samples
            n_needed = min_samples - len(class_df)

            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å samples ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            synthetic_samples = []
            for _ in range(n_needed):
                base_sample = class_df.sample(1).iloc[0].copy()

                # ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô features
                for col in feature_cols:
                    if df_fixed[col].dtype in ['int64', 'float64']:
                        noise = np.random.normal(0, abs(base_sample[col]) * 0.01)
                        base_sample[col] += noise

                synthetic_samples.append(base_sample)

            if synthetic_samples:
                synthetic_df = pd.DataFrame(synthetic_samples)
                class_df = pd.concat([class_df, synthetic_df], ignore_index = True)
                print(f"‚úÖ Created {n_needed} synthetic samples for class {class_val}")

        balanced_dfs.append(class_df)

    df_balanced = pd.concat(balanced_dfs, ignore_index = True).sample(frac = 1).reset_index(drop = True)
    print(f"‚úÖ Balanced dataset: {df_balanced[target_col].value_counts().to_dict()}")

    return df_balanced

def test_models_with_fixes(df, target_col = 'target'):
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö models ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"""
    print("\nüß™ Testing models with fixes...")

    try:
    except ImportError as e:
        print(f"‚ùå Missing sklearn: {e}")
        return {}

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° data
    feature_cols = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
    X = df[feature_cols]
    y = df[target_col]

    print(f"üìä Features: {len(feature_cols)}, Samples: {len(df)}")
    print(f"üìä Target distribution: {y.value_counts().to_dict()}")

    # Scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # ‡πÅ‡∏ö‡πà‡∏á train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size = 0.3, random_state = 42, stratify = y
    )

    # Models with aggressive parameters
    models = {
        'Random Forest (Balanced)': RandomForestClassifier(
            n_estimators = 50, 
            max_depth = 5, 
            class_weight = 'balanced', 
            random_state = 42, 
            min_samples_split = 5, 
            min_samples_leaf = 2
        ), 
        'Logistic Regression (Balanced)': LogisticRegression(
            class_weight = 'balanced', 
            random_state = 42, 
            max_iter = 1000, 
            solver = 'liblinear'
        )
    }

    results = {}

    for name, model in models.items():
        try:
            print(f"\nüîÑ Testing {name}...")

            # Fit model
            model.fit(X_train, y_train)

            # Predictions
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)

            # Metrics
            if len(np.unique(y)) > 2:  # Multi - class
                auc = roc_auc_score(y_test, y_pred_proba, multi_class = 'ovr', average = 'macro')
            else:  # Binary
                auc = roc_auc_score(y_test, y_pred_proba[:, 1])

            results[name] = {
                'auc': auc, 
                'predictions': len(y_pred), 
                'classes': len(np.unique(y_pred))
            }

            print(f"‚úÖ {name}: AUC = {auc:.4f}")

            # Classification report
            report = classification_report(y_test, y_pred, output_dict = True, zero_division = 0)
            print(f"üìä Accuracy: {report['accuracy']:.4f}")

        except Exception as e:
            print(f"‚ùå {name} failed: {e}")
            results[name] = {'auc': np.nan, 'error': str(e)}

    return results

def main():
    """Main execution function"""
    print("üö® EMERGENCY NaN AUC FIX - STARTING")
    print(" = " * 50)

    output_dir = create_output_dir()

    try:
        # 1. Load data
        df = load_test_data()

        # 2. Diagnose issues
        issues = diagnose_nan_auc_causes(df)

        print(f"\nüîç Found {len(issues)} critical issues:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")

        # 3. Apply fixes
        df_fixed = apply_aggressive_fixes(df)

        # 4. Test models
        results = test_models_with_fixes(df_fixed)

        # 5. Save results
        report = {
            'timestamp': datetime.now().isoformat(), 
            'original_shape': df.shape, 
            'fixed_shape': df_fixed.shape, 
            'issues_found': issues, 
            'model_results': results, 
            'status': 'completed'
        }

        # Save report
        report_file = output_dir / 'emergency_nan_auc_fix_report.json'
        with open(report_file, 'w') as f:
            json.dump(report, f, indent = 2, default = str)

        # Save fixed data
        data_file = output_dir / 'emergency_fixed_data.csv'
        df_fixed.to_csv(data_file, index = False)

        print(f"\nüìÅ Results saved:")
        print(f"  üìã Report: {report_file}")
        print(f"  üìä Data: {data_file}")

        # Summary
        print("\n" + " = "*50)
        print("üéØ EMERGENCY FIX SUMMARY")
        print(" = "*50)

        successful_models = [name for name, result in results.items()
                           if isinstance(result.get('auc'), (int, float)) and not np.isnan(result['auc'])]

        if successful_models:
            print("‚úÖ SUCCESS: Fixed NaN AUC issue!")
            for name in successful_models:
                auc = results[name]['auc']
                print(f"  üéØ {name}: AUC = {auc:.4f}")
        else:
            print("‚ùå FAILED: Still getting NaN AUC")
            print("üí° Recommendations:")
            print("  1. Check if you have sklearn installed")
            print("  2. Try with different data")
            print("  3. Check for environment issues")

        print(f"\nüìä Issues addressed: {len(issues)}")
        print(f"üìà Data enhanced: {df.shape} ‚Üí {df_fixed.shape}")

    except Exception as e:
        print(f"üí• EMERGENCY FIX FAILED: {e}")
        traceback.print_exc()

        # Save error report
        error_report = {
            'timestamp': datetime.now().isoformat(), 
            'error': str(e), 
            'traceback': traceback.format_exc(), 
            'status': 'failed'
        }

        error_file = output_dir / 'emergency_fix_error.json'
        with open(error_file, 'w') as f:
            json.dump(error_report, f, indent = 2)

        print(f"üíæ Error report saved: {error_file}")

if __name__ == "__main__":
    main()