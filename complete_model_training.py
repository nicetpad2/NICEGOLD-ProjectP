#!/usr/bin/env python3
"""
üéØ COMPLETE MODEL TRAINING SCRIPT
‡∏™‡∏£‡πâ‡∏≤‡∏á models ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö project ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Model file not found: models\rf_model.joblib
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
import joblib
from datetime import datetime
import os

warnings.filterwarnings('ignore')

def create_necessary_dirs():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á directories ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    dirs = ['models', 'output_default', 'data']
    for dir_name in dirs:
        Path(dir_name).mkdir(exist_ok=True)
        print(f"‚úÖ Created/verified directory: {dir_name}")

def load_or_create_training_data():
    """‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á training data"""
    print("üìÅ Loading training data...")
    
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ data files ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    data_files = [
        "XAUUSD_M1.csv", "XAUUSD_M15.csv",
        "dummy_m1.csv", "dummy_m15.csv",
        "data/XAUUSD_M1.csv", "data/XAUUSD_M15.csv"
    ]
    
    df = None
    for file_path in data_files:
        if Path(file_path).exists():
            try:
                df = pd.read_csv(file_path)
                print(f"‚úÖ Loaded data from {file_path}: {df.shape}")
                print(f"üìã Columns: {list(df.columns)}")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {file_path}: {e}")
    
    if df is None:
        print("üîß Creating synthetic training data...")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic data ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö trading
        np.random.seed(42)
        n_samples = 10000
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏ö‡∏ö realistic
        prices = []
        initial_price = 2000  # Gold price
        price = initial_price
        
        for _ in range(n_samples):
            # Random walk with trend
            change = np.random.randn() * 0.5 + np.random.choice([-0.1, 0.1], p=[0.5, 0.5])
            price += change
            prices.append(price)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á OHLCV data
        df = pd.DataFrame({
            'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='1T'),
            'Open': prices,
            'High': [p + abs(np.random.randn() * 0.5) for p in prices],
            'Low': [p - abs(np.random.randn() * 0.5) for p in prices],
            'Close': [p + np.random.randn() * 0.2 for p in prices],
            'Volume': np.random.exponential(1000, n_samples)
        })
        
        print(f"‚úÖ Created synthetic data: {df.shape}")
    
    return df

def engineer_features(df):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö trading"""
    print("üîß Engineering features...")
    
    df_processed = df.copy()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö columns ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    required_cols = ['Close']
    price_col = None
    
    # ‡∏´‡∏≤ price column
    for col in ['Close', 'close', 'price', 'Close']:
        if col in df.columns:
            price_col = col
            break
    
    if price_col is None:
        # ‡πÉ‡∏ä‡πâ column ‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô price
        price_col = df.columns[0]
        print(f"‚ö†Ô∏è Using {price_col} as price column")
    
    # Technical indicators
    df_processed['returns'] = df_processed[price_col].pct_change().fillna(0)
    df_processed['volatility'] = df_processed['returns'].rolling(20, min_periods=1).std().fillna(0)
    
    # Moving averages
    df_processed['sma_5'] = df_processed[price_col].rolling(5, min_periods=1).mean()
    df_processed['sma_20'] = df_processed[price_col].rolling(20, min_periods=1).mean()
    
    # RSI approximation
    price_changes = df_processed[price_col].diff()
    gains = price_changes.where(price_changes > 0, 0)
    losses = -price_changes.where(price_changes < 0, 0)
    avg_gains = gains.rolling(14, min_periods=1).mean()
    avg_losses = losses.rolling(14, min_periods=1).mean()
    rs = avg_gains / (avg_losses + 1e-8)
    df_processed['rsi'] = 100 - (100 / (1 + rs))
    
    # MACD approximation
    ema_12 = df_processed[price_col].ewm(span=12).mean()
    ema_26 = df_processed[price_col].ewm(span=26).mean()
    df_processed['macd'] = ema_12 - ema_26
    
    # Momentum
    df_processed['momentum'] = df_processed[price_col].pct_change(10).fillna(0)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° columns ‡∏ó‡∏µ‡πà pipeline ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    if 'Open' not in df_processed.columns:
        df_processed['Open'] = df_processed[price_col] * (1 + np.random.randn(len(df_processed)) * 0.001)
    
    if 'Volume' not in df_processed.columns:
        df_processed['Volume'] = np.random.exponential(1000, len(df_processed))
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á target variable (trading signal)
    # ‡πÉ‡∏ä‡πâ future returns ‡πÄ‡∏õ‡πá‡∏ô signal
    future_returns = df_processed['returns'].shift(-1).fillna(0)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á target: 1 for buy, -1 for sell, 0 for hold
    percentile_75 = future_returns.quantile(0.75)
    percentile_25 = future_returns.quantile(0.25)
    
    df_processed['target'] = np.where(future_returns > percentile_75, 1,
                                     np.where(future_returns < percentile_25, -1, 0))
    
    # ‡∏•‡∏ö NaN
    df_processed = df_processed.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    print(f"‚úÖ Feature engineering completed: {df_processed.shape}")
    print(f"üìä Target distribution: {df_processed['target'].value_counts().to_dict()}")
    
    return df_processed

def train_and_save_models(df):
    """Train ‡πÅ‡∏•‡∏∞ save models ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    print("ü§ñ Training models...")
    
    try:
        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.preprocessing import StandardScaler
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import classification_report, roc_auc_score
    except ImportError as e:
        print(f"‚ùå Missing sklearn: {e}")
        print("Please install: pip install scikit-learn")
        return False
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features ‡πÅ‡∏•‡∏∞ target
    feature_cols = ['Open', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd']
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ features ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    available_features = [col for col in feature_cols if col in df.columns]
    if len(available_features) < 3:
        print(f"‚ö†Ô∏è Not enough features available: {available_features}")
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° features ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î
        for col in feature_cols:
            if col not in df.columns:
                df[col] = np.random.randn(len(df)) * 0.1
        available_features = feature_cols
    
    X = df[available_features]
    y = df['target']
    
    print(f"üìä Training data: {X.shape}, Target classes: {y.value_counts().to_dict()}")
    
    # ‡πÅ‡∏ö‡πà‡∏á train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # Scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Models to train
    models = {
        'rf_model.joblib': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        ),
        'gb_model.joblib': GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        ),
        'lr_model.joblib': LogisticRegression(
            class_weight='balanced',
            random_state=42,
            max_iter=1000
        )
    }
    
    trained_models = {}
    
    for model_name, model in models.items():
        try:
            print(f"\nüîÑ Training {model_name}...")
            
            # Train model
            if 'lr_' in model_name:
                model.fit(X_train_scaled, y_train)
                y_pred_proba = model.predict_proba(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred_proba = model.predict_proba(X_test)
            
            # Calculate metrics
            y_pred = model.predict(X_test_scaled if 'lr_' in model_name else X_test)
            
            # AUC score
            if len(np.unique(y)) > 2:  # Multi-class
                auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
            else:  # Binary (convert to binary)
                y_binary = (y_test != 0).astype(int)
                y_pred_binary_proba = y_pred_proba[:, 1:].sum(axis=1) if y_pred_proba.shape[1] > 2 else y_pred_proba[:, 1]
                auc = roc_auc_score(y_binary, y_pred_binary_proba)
            
            # Save model
            model_path = Path('models') / model_name
            joblib.dump(model, model_path)
            
            # Save scaler for lr model
            if 'lr_' in model_name:
                scaler_path = Path('models') / f"scaler_{model_name}"
                joblib.dump(scaler, scaler_path)
            
            trained_models[model_name] = {
                'auc': auc,
                'model_path': str(model_path),
                'feature_cols': available_features
            }
            
            print(f"‚úÖ {model_name} trained successfully!")
            print(f"   üìä AUC Score: {auc:.4f}")
            print(f"   üíæ Saved to: {model_path}")
            
        except Exception as e:
            print(f"‚ùå Failed to train {model_name}: {e}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å feature order
    feature_order_file = Path('output_default') / 'train_features.txt'
    with open(feature_order_file, 'w') as f:
        for feature in available_features:
            f.write(f"{feature}\n")
    print(f"‚úÖ Feature order saved to: {feature_order_file}")
    
    return trained_models

def create_sample_config():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á config file ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"""
    print("‚öôÔ∏è Creating sample configuration...")
    
    config = {
        'model': {
            'type': 'RandomForest',
            'file': 'models/rf_model.joblib',
            'features': ['Open', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd']
        },
        'data': {
            'source': 'dummy_m1.csv',
            'target_column': 'target'
        },
        'training': {
            'test_size': 0.3,
            'random_state': 42,
            'cross_validation': 5
        }
    }
    
    config_file = Path('config.yaml')
    try:
        import yaml
        with open(config_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"‚úÖ Config saved to: {config_file}")
    except ImportError:
        # Fallback to JSON
        config_file = Path('config.json')
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"‚úÖ Config saved to: {config_file}")

def main():
    """Main execution"""
    print("üéØ COMPLETE MODEL TRAINING - STARTING")
    print("=" * 60)
    
    try:
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á directories
        create_necessary_dirs()
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á data
        df = load_or_create_training_data()
        
        # 3. Feature engineering
        df_processed = engineer_features(df)
        
        # 4. Train models
        models_trained = train_and_save_models(df_processed)
        
        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á config
        create_sample_config()
        
        # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å processed data
        processed_data_file = Path('output_default') / 'preprocessed_super.parquet'
        df_processed.to_parquet(processed_data_file)
        print(f"‚úÖ Processed data saved to: {processed_data_file}")
        
        # Summary
        print("\n" + "=" * 60)
        print("üéØ TRAINING COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        
        print(f"‚úÖ Models trained: {len(models_trained)}")
        for model_name, details in models_trained.items():
            print(f"  ü§ñ {model_name}: AUC = {details['auc']:.4f}")
        
        print(f"\nüìÅ Files created:")
        print(f"  üóÇÔ∏è Models folder: models/")
        print(f"  üìä Processed data: output_default/preprocessed_super.parquet") 
        print(f"  üìã Feature order: output_default/train_features.txt")
        print(f"  ‚öôÔ∏è Config file: config.yaml/config.json")
        
        print(f"\nüéâ PROJECT IS NOW READY TO USE!")
        print(f"You can now run the main pipeline without model errors.")
        
        return True
        
    except Exception as e:
        print(f"üí• TRAINING FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\n‚úÖ All models and dependencies created successfully!")
        print("üöÄ Your project is now complete and ready to use!")
    else:
        print("\n‚ùå Training failed. Please check the errors above.")
