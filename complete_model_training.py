#!/usr/bin/env python3
"""
ğŸ¯ COMPLETE MODEL TRAINING SCRIPT
à¸ªà¸£à¹‰à¸²à¸‡ models à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸š project à¹ƒà¸«à¹‰à¸„à¸£à¸šà¸–à¹‰à¸§à¸™
à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸²: Model file not found: models\rf_model.joblib
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
import joblib
from datetime import datetime
import os

warnings.filterwarnings('ignore')

def create_necessary_dirs():
    """à¸ªà¸£à¹‰à¸²à¸‡ directories à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™"""
    dirs = ['models', 'output_default', 'data']
    for dir_name in dirs:
        Path(dir_name).mkdir(exist_ok=True)
        print(f"âœ… Created/verified directory: {dir_name}")

def load_or_create_training_data():
    """à¹‚à¸«à¸¥à¸”à¸«à¸£à¸·à¸­à¸ªà¸£à¹‰à¸²à¸‡ training data"""
    print("ğŸ“ Loading training data...")
    
    # à¸¥à¸­à¸‡à¸«à¸² data files à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆ
    data_files = [
        "XAUUSD_M1.csv", "XAUUSD_M15.csv",
        "dummy_m1.csv", "dummy_m15.csv",
        "data/XAUUSD_M1.csv", "data/XAUUSD_M15.csv"
    ]
    
    df = None
    for file_path in data_files:
        if Path(file_path).exists():
            try:
                df = pd.read_csv(file_path)
                print(f"âœ… Loaded data from {file_path}: {df.shape}")
                print(f"ğŸ“‹ Columns: {list(df.columns)}")
                break
            except Exception as e:
                print(f"âš ï¸ Failed to load {file_path}: {e}")
    
    if df is None:
        print("ğŸ”§ Creating synthetic training data...")
        # à¸ªà¸£à¹‰à¸²à¸‡ synthetic data à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸š trading
        np.random.seed(42)
        n_samples = 10000
        
        # à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸„à¸²à¹à¸šà¸š realistic
        prices = []
        initial_price = 2000  # Gold price
        price = initial_price
        
        for _ in range(n_samples):
            # Random walk with trend
            change = np.random.randn() * 0.5 + np.random.choice([-0.1, 0.1], p=[0.5, 0.5])
            price += change
            prices.append(price)
        
        # à¸ªà¸£à¹‰à¸²à¸‡ OHLCV data
        df = pd.DataFrame({
            'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='1T'),
            'Open': prices,
            'High': [p + abs(np.random.randn() * 0.5) for p in prices],
            'Low': [p - abs(np.random.randn() * 0.5) for p in prices],
            'Close': [p + np.random.randn() * 0.2 for p in prices],
            'Volume': np.random.exponential(1000, n_samples)
        })
        
        print(f"âœ… Created synthetic data: {df.shape}")
    
    return df

def engineer_features(df):
    """à¸ªà¸£à¹‰à¸²à¸‡ features à¸ªà¸³à¸«à¸£à¸±à¸š trading"""
    print("ğŸ”§ Engineering features...")
    
    df_processed = df.copy()
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š columns à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™
    required_cols = ['Close']
    price_col = None
    
    # à¸«à¸² price column
    for col in ['Close', 'close', 'price', 'Close']:
        if col in df.columns:
            price_col = col
            break
    
    if price_col is None:
        # à¹ƒà¸Šà¹‰ column à¹à¸£à¸à¹€à¸›à¹‡à¸™ price
        price_col = df.columns[0]
        print(f"âš ï¸ Using {price_col} as price column")
    
    # Technical indicators
    df_processed['returns'] = df_processed[price_col].pct_change().fillna(0)
    df_processed['volatility'] = df_processed['returns'].rolling(20, min_periods=1).std().fillna(0)
    
    # Moving averages
    df_processed['sma_5'] = df_processed[price_col].rolling(5, min_periods=1).mean()
    df_processed['sma_20'] = df_processed[price_col].rolling(20, min_periods=1).mean()
    
    # RSI approximation
    price_changes = df_processed[price_col].diff()
    gains = price_changes.where(price_changes > 0, 0)
    losses = -price_changes.where(price_changes < 0, 0)
    avg_gains = gains.rolling(14, min_periods=1).mean()
    avg_losses = losses.rolling(14, min_periods=1).mean()
    rs = avg_gains / (avg_losses + 1e-8)
    df_processed['rsi'] = 100 - (100 / (1 + rs))
    
    # MACD approximation
    ema_12 = df_processed[price_col].ewm(span=12).mean()
    ema_26 = df_processed[price_col].ewm(span=26).mean()
    df_processed['macd'] = ema_12 - ema_26
    
    # Momentum
    df_processed['momentum'] = df_processed[price_col].pct_change(10).fillna(0)
    
    # à¹€à¸à¸´à¹ˆà¸¡ columns à¸—à¸µà¹ˆ pipeline à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    if 'Open' not in df_processed.columns:
        df_processed['Open'] = df_processed[price_col] * (1 + np.random.randn(len(df_processed)) * 0.001)
    
    if 'Volume' not in df_processed.columns:
        df_processed['Volume'] = np.random.exponential(1000, len(df_processed))
    
    # à¸ªà¸£à¹‰à¸²à¸‡ target variable (trading signal)
    # à¹ƒà¸Šà¹‰ future returns à¹€à¸›à¹‡à¸™ signal
    future_returns = df_processed['returns'].shift(-1).fillna(0)
    
    # à¸ªà¸£à¹‰à¸²à¸‡ target: 1 for buy, -1 for sell, 0 for hold
    percentile_75 = future_returns.quantile(0.75)
    percentile_25 = future_returns.quantile(0.25)
    
    df_processed['target'] = np.where(future_returns > percentile_75, 1,
                                     np.where(future_returns < percentile_25, -1, 0))
    
    # à¸¥à¸š NaN
    df_processed = df_processed.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    print(f"âœ… Feature engineering completed: {df_processed.shape}")
    print(f"ğŸ“Š Target distribution: {df_processed['target'].value_counts().to_dict()}")
    
    return df_processed

def train_and_save_models(df):
    """Train à¹à¸¥à¸° save models à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™"""
    print("ğŸ¤– Training models...")
    
    try:
        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.preprocessing import StandardScaler
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import classification_report, roc_auc_score
    except ImportError as e:
        print(f"âŒ Missing sklearn: {e}")
        print("Please install: pip install scikit-learn")
        return False
    
    # à¹€à¸•à¸£à¸µà¸¢à¸¡ features à¹à¸¥à¸° target
    feature_cols = ['Open', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd']
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µ features à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™
    available_features = [col for col in feature_cols if col in df.columns]
    if len(available_features) < 3:
        print(f"âš ï¸ Not enough features available: {available_features}")
        # à¹€à¸à¸´à¹ˆà¸¡ features à¸—à¸µà¹ˆà¸‚à¸²à¸”
        for col in feature_cols:
            if col not in df.columns:
                df[col] = np.random.randn(len(df)) * 0.1
        available_features = feature_cols
    
    X = df[available_features]
    y = df['target']
    
    print(f"ğŸ“Š Training data: {X.shape}, Target classes: {y.value_counts().to_dict()}")
    
    # à¹à¸šà¹ˆà¸‡ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # Scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Models to train
    models = {
        'rf_model.joblib': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        ),
        'gb_model.joblib': GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        ),
        'lr_model.joblib': LogisticRegression(
            class_weight='balanced',
            random_state=42,
            max_iter=1000
        )
    }
    
    trained_models = {}
    
    for model_name, model in models.items():
        try:
            print(f"\nğŸ”„ Training {model_name}...")
            
            # Train model
            if 'lr_' in model_name:
                model.fit(X_train_scaled, y_train)
                y_pred_proba = model.predict_proba(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred_proba = model.predict_proba(X_test)
            
            # Calculate metrics
            y_pred = model.predict(X_test_scaled if 'lr_' in model_name else X_test)
            
            # AUC score
            if len(np.unique(y)) > 2:  # Multi-class
                auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
            else:  # Binary (convert to binary)
                y_binary = (y_test != 0).astype(int)
                y_pred_binary_proba = y_pred_proba[:, 1:].sum(axis=1) if y_pred_proba.shape[1] > 2 else y_pred_proba[:, 1]
                auc = roc_auc_score(y_binary, y_pred_binary_proba)
            
            # Save model
            model_path = Path('models') / model_name
            joblib.dump(model, model_path)
            
            # Save scaler for lr model
            if 'lr_' in model_name:
                scaler_path = Path('models') / f"scaler_{model_name}"
                joblib.dump(scaler, scaler_path)
            
            trained_models[model_name] = {
                'auc': auc,
                'model_path': str(model_path),
                'feature_cols': available_features
            }
            
            print(f"âœ… {model_name} trained successfully!")
            print(f"   ğŸ“Š AUC Score: {auc:.4f}")
            print(f"   ğŸ’¾ Saved to: {model_path}")
            
        except Exception as e:
            print(f"âŒ Failed to train {model_name}: {e}")
    
    # à¸šà¸±à¸™à¸—à¸¶à¸ feature order
    feature_order_file = Path('output_default') / 'train_features.txt'
    with open(feature_order_file, 'w') as f:
        for feature in available_features:
            f.write(f"{feature}\n")
    print(f"âœ… Feature order saved to: {feature_order_file}")
    
    return trained_models

def create_sample_config():
    """à¸ªà¸£à¹‰à¸²à¸‡ config file à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡"""
    print("âš™ï¸ Creating sample configuration...")
    
    config = {
        'model': {
            'type': 'RandomForest',
            'file': 'models/rf_model.joblib',
            'features': ['Open', 'Volume', 'returns', 'volatility', 'momentum', 'rsi', 'macd']
        },
        'data': {
            'source': 'dummy_m1.csv',
            'target_column': 'target'
        },
        'training': {
            'test_size': 0.3,
            'random_state': 42,
            'cross_validation': 5
        }
    }
    
    config_file = Path('config.yaml')
    try:
        import yaml
        with open(config_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"âœ… Config saved to: {config_file}")
    except ImportError:
        # Fallback to JSON
        config_file = Path('config.json')
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"âœ… Config saved to: {config_file}")

def main():
    """Main execution"""
    print("ğŸ¯ COMPLETE MODEL TRAINING - STARTING")
    print("=" * 60)
    
    try:
        # 1. à¸ªà¸£à¹‰à¸²à¸‡ directories
        create_necessary_dirs()
        
        # 2. à¹‚à¸«à¸¥à¸”à¸«à¸£à¸·à¸­à¸ªà¸£à¹‰à¸²à¸‡ data
        df = load_or_create_training_data()
        
        # 3. Feature engineering
        df_processed = engineer_features(df)
        
        # 4. Train models
        models_trained = train_and_save_models(df_processed)
        
        # 5. à¸ªà¸£à¹‰à¸²à¸‡ config
        create_sample_config()
        
        # 6. à¸šà¸±à¸™à¸—à¸¶à¸ processed data
        processed_data_file = Path('output_default') / 'preprocessed_super.parquet'
        df_processed.to_parquet(processed_data_file)
        print(f"âœ… Processed data saved to: {processed_data_file}")
        
        # Summary
        print("\n" + "=" * 60)
        print("ğŸ¯ TRAINING COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        
        print(f"âœ… Models trained: {len(models_trained)}")
        for model_name, details in models_trained.items():
            print(f"  ğŸ¤– {model_name}: AUC = {details['auc']:.4f}")
        
        print(f"\nğŸ“ Files created:")
        print(f"  ğŸ—‚ï¸ Models folder: models/")
        print(f"  ğŸ“Š Processed data: output_default/preprocessed_super.parquet") 
        print(f"  ğŸ“‹ Feature order: output_default/train_features.txt")
        print(f"  âš™ï¸ Config file: config.yaml/config.json")
        
        print(f"\nğŸ‰ PROJECT IS NOW READY TO USE!")
        print(f"You can now run the main pipeline without model errors.")
        
        return True
        
    except Exception as e:
        print(f"ğŸ’¥ TRAINING FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… All models and dependencies created successfully!")
        print("ğŸš€ Your project is now complete and ready to use!")
    else:
        print("\nâŒ Training failed. Please check the errors above.")
