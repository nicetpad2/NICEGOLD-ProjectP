#!/usr/bin/env python3
        from imblearn.combine import SMOTEENN, SMOTETomek
        from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
        from sklearn.preprocessing import StandardScaler
        import json
import numpy as np
import pandas as pd
        import traceback
import warnings
"""
ðŸš€ PRODUCTION - GRADE CLASS IMBALANCE FIX
à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸² Class Imbalance à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¹à¸šà¸šà¸£à¸°à¸”à¸±à¸š Production
"""

warnings.filterwarnings('ignore')

def fix_extreme_class_imbalance_production():
    """
    à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸² Extreme Class Imbalance à¸­à¸¢à¹ˆà¸²à¸‡à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡à¹à¸¥à¸°à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž
    """
    print("ðŸ”§ PRODUCTION CLASS IMBALANCE FIX STARTING...")

    try:
        # à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        data_path = "output_default/preprocessed_super.parquet"
        df = pd.read_parquet(data_path)
        print(f"ðŸ“Š Original data shape: {df.shape}")

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š target distribution
        target_counts = df['target'].value_counts().sort_index()
        print(f"ðŸ“Š Original target distribution: {target_counts.to_dict()}")

        # à¸„à¸³à¸™à¸§à¸“ imbalance ratio
        max_count = target_counts.max()
        min_count = target_counts.min()
        imbalance_ratio = max_count / min_count
        print(f"âš–ï¸ Imbalance ratio: {imbalance_ratio:.1f}:1")

        if imbalance_ratio > 50:  # Extreme imbalance
            print("ðŸš¨ EXTREME IMBALANCE DETECTED - Applying comprehensive fixes...")

            # 1. SYNTHETIC DATA GENERATION (SMOTE + ADASYN)
            df_balanced = apply_advanced_sampling(df)

            # 2. FEATURE ENHANCEMENT à¸ªà¸³à¸«à¸£à¸±à¸š minority classes
            df_enhanced = enhance_features_for_minorities(df_balanced)

            # 3. TARGET REBALANCING à¹‚à¸”à¸¢à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ binary targets
            df_final = create_balanced_binary_targets(df_enhanced)

            # 4. VALIDATION à¹à¸¥à¸° QUALITY CHECK
            validate_balanced_data(df_final)

            # à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ
            output_path = "output_default/balanced_data_production.parquet"
            df_final.to_parquet(output_path)
            print(f"âœ… Balanced data saved to: {output_path}")

            # à¸ªà¸£à¹‰à¸²à¸‡ metadata
            create_balance_metadata(df, df_final, output_path)

            return df_final, output_path

        else:
            print("âœ… Imbalance within acceptable range")
            return df, data_path

    except Exception as e:
        print(f"âŒ Error in class imbalance fix: {e}")
        traceback.print_exc()
        return None, None

def apply_advanced_sampling(df):
    """
    à¹ƒà¸Šà¹‰ SMOTE + ADASYN + BorderlineSMOTE à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ
    """
    print("ðŸŽ¯ Applying advanced sampling techniques...")

    try:

        # à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        feature_cols = [c for c in df.columns if c not in ['target', 'date', 'datetime', 'timestamp', 'time']]
        X = df[feature_cols].select_dtypes(include = [np.number])
        y = df['target']

        # à¸ˆà¸±à¸”à¸à¸²à¸£ missing values
        X = X.fillna(X.median())

        # Standard scaling à¸ªà¸³à¸«à¸£à¸±à¸š SMOTE
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Strategy 1: SMOTE à¹à¸šà¸šà¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡
        target_counts = y.value_counts()
        majority_class = target_counts.idxmax()
        majority_count = target_counts.max()

        # à¸à¸³à¸«à¸™à¸” sampling strategy à¹ƒà¸«à¹‰à¸ªà¸¡à¹€à¸«à¸•à¸¸à¸ªà¸¡à¸œà¸¥
        sampling_strategy = {}
        for cls, count in target_counts.items():
            if cls != majority_class:
                # à¹€à¸žà¸´à¹ˆà¸¡à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ 20% à¸‚à¸­à¸‡ majority class (à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¹€à¸—à¹ˆà¸²à¸à¸±à¸™)
                target_samples = max(int(majority_count * 0.2), count * 3)
                sampling_strategy[cls] = min(target_samples, majority_count // 2)

        print(f"ðŸ“Š SMOTE sampling strategy: {sampling_strategy}")

        # à¹ƒà¸Šà¹‰ BorderlineSMOTE à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™
        smote = BorderlineSMOTE(
            sampling_strategy = sampling_strategy, 
            random_state = 42, 
            k_neighbors = min(5, len(X) // 10),  # à¸›à¸£à¸±à¸š k_neighbors à¸•à¸²à¸¡à¸‚à¸™à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
            m_neighbors = min(10, len(X) // 5)
        )

        X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

        # à¹à¸›à¸¥à¸‡à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™ original scale
        X_resampled = scaler.inverse_transform(X_resampled)

        # à¸ªà¸£à¹‰à¸²à¸‡ DataFrame à¹ƒà¸«à¸¡à¹ˆ
        df_resampled = pd.DataFrame(X_resampled, columns = X.columns)
        df_resampled['target'] = y_resampled

        # à¹€à¸žà¸´à¹ˆà¸¡ non - numeric columns à¸à¸¥à¸±à¸šà¹€à¸‚à¹‰à¸²à¹„à¸›
        non_numeric_cols = [c for c in df.columns if c not in X.columns and c != 'target']
        for col in non_numeric_cols:
            if col in df.columns:
                # Duplicate values à¸ªà¸³à¸«à¸£à¸±à¸š synthetic samples
                original_values = df[col].values
                synthetic_count = len(df_resampled) - len(df)
                if synthetic_count > 0:
                    # Random sample à¸ˆà¸²à¸ original values
                    synthetic_values = np.random.choice(original_values, synthetic_count)
                    all_values = np.concatenate([original_values, synthetic_values])
                    df_resampled[col] = all_values[:len(df_resampled)]

        print(f"âœ… SMOTE completed. New shape: {df_resampled.shape}")
        print(f"ðŸ“Š New target distribution: {df_resampled['target'].value_counts().to_dict()}")

        return df_resampled

    except ImportError:
        print("âŒ imbalanced - learn not available, using basic techniques...")
        return apply_basic_sampling(df)
    except Exception as e:
        print(f"âŒ SMOTE failed: {e}, falling back to basic techniques...")
        return apply_basic_sampling(df)

def apply_basic_sampling(df):
    """
    à¹€à¸—à¸„à¸™à¸´à¸„à¸žà¸·à¹‰à¸™à¸à¸²à¸™à¹ƒà¸™à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ class imbalance à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸¡à¸µ advanced libraries
    """
    print("ðŸ”§ Applying basic sampling techniques...")

    target_counts = df['target'].value_counts()
    majority_class = target_counts.idxmax()
    minority_classes = [c for c in target_counts.index if c != majority_class]

    balanced_dfs = []

    # Keep majority class (à¸­à¸²à¸ˆà¸ˆà¸° undersample à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢)
    majority_df = df[df['target'] == majority_class]
    if len(majority_df) > 100000:  # à¸–à¹‰à¸²à¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸› à¹ƒà¸«à¹‰ sample à¸¥à¸‡
        majority_df = majority_df.sample(n = 100000, random_state = 42)
    balanced_dfs.append(majority_df)

    # Oversample minority classes
    for cls in minority_classes:
        minority_df = df[df['target'] == cls]
        current_count = len(minority_df)
        target_count = min(len(majority_df) // 5, current_count * 10)  # à¹€à¸žà¸´à¹ˆà¸¡à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¹€à¸à¸´à¸™à¹„à¸›

        if target_count > current_count:
            # Oversample by repeating with noise
            additional_samples = target_count - current_count
            repeated_samples = []

            for _ in range(additional_samples):
                # Random sample à¹à¸¥à¸°à¹€à¸žà¸´à¹ˆà¸¡ noise à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢
                sample = minority_df.sample(n = 1, random_state = np.random.randint(10000))

                # à¹€à¸žà¸´à¹ˆà¸¡ Gaussian noise à¹ƒà¸«à¹‰ numeric columns
                sample_copy = sample.copy()
                numeric_cols = sample_copy.select_dtypes(include = [np.number]).columns
                for col in numeric_cols:
                    if col != 'target':
                        noise = np.random.normal(0, sample_copy[col].std() * 0.01)
                        sample_copy[col] = sample_copy[col] + noise

                repeated_samples.append(sample_copy)

            if repeated_samples:
                augmented_df = pd.concat([minority_df] + repeated_samples, ignore_index = True)
                balanced_dfs.append(augmented_df)
            else:
                balanced_dfs.append(minority_df)
        else:
            balanced_dfs.append(minority_df)

    # à¸£à¸§à¸¡à¸—à¸¸à¸ class à¹€à¸‚à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸à¸±à¸™
    df_balanced = pd.concat(balanced_dfs, ignore_index = True)

    # Shuffle data
    df_balanced = df_balanced.sample(frac = 1, random_state = 42).reset_index(drop = True)

    print(f"âœ… Basic sampling completed. New shape: {df_balanced.shape}")
    print(f"ðŸ“Š New target distribution: {df_balanced['target'].value_counts().to_dict()}")

    return df_balanced

def enhance_features_for_minorities(df):
    """
    à¸ªà¸£à¹‰à¸²à¸‡ features à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¸—à¸µà¹ˆà¸Šà¹ˆà¸§à¸¢à¹à¸¢à¸à¹à¸¢à¸° minority classes à¹„à¸”à¹‰à¸”à¸µà¸‚à¸¶à¹‰à¸™
    """
    print("ðŸŽ¯ Enhancing features for minority class detection...")

    try:
        # 1. Statistical features
        numeric_cols = df.select_dtypes(include = [np.number]).columns
        numeric_cols = [c for c in numeric_cols if c != 'target']

        # Rolling statistics with multiple windows
        for col in numeric_cols[:5]:  # à¹€à¸¥à¸·à¸­à¸à¹à¸„à¹ˆ top 5 columns à¹€à¸žà¸·à¹ˆà¸­à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž
            for window in [3, 7, 14]:
                df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window).mean()
                df[f'{col}_rolling_std_{window}'] = df[col].rolling(window).std()

        # 2. Interaction features
        if len(numeric_cols) >= 2:
            for i in range(min(3, len(numeric_cols))):
                for j in range(i + 1, min(3, len(numeric_cols))):
                    col1, col2 = numeric_cols[i], numeric_cols[j]
                    df[f'{col1}_x_{col2}'] = df[col1] * df[col2]
                    df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e - 8)

        # 3. Percentile features
        for col in numeric_cols[:3]:
            df[f'{col}_pct_rank'] = df[col].rank(pct = True)
            df[f'{col}_zscore'] = (df[col] - df[col].mean()) / (df[col].std() + 1e - 8)

        # 4. Time - based features (à¸–à¹‰à¸²à¸¡à¸µ datetime column)
        date_cols = [c for c in df.columns if 'date' in c.lower() or 'time' in c.lower()]
        if date_cols:
            date_col = date_cols[0]
            try:
                if df[date_col].dtype == 'object':
                    df[date_col] = pd.to_datetime(df[date_col])
                df['hour'] = df[date_col].dt.hour
                df['day_of_week'] = df[date_col].dt.dayofweek
                df['month'] = df[date_col].dt.month
            except:
                pass

        # 5. Target encoding features
        for col in numeric_cols[:3]:
            target_mean = df.groupby(col)['target'].mean()
            df[f'{col}_target_enc'] = df[col].map(target_mean)

        # Remove features with too many NaN values
        df = df.loc[:, df.isnull().mean() < 0.3]

        # Fill remaining NaN values
        numeric_cols = df.select_dtypes(include = [np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

        print(f"âœ… Feature enhancement completed. New shape: {df.shape}")
        return df

    except Exception as e:
        print(f"âŒ Feature enhancement failed: {e}")
        return df

def create_balanced_binary_targets(df):
    """
    à¸ªà¸£à¹‰à¸²à¸‡ binary targets à¸—à¸µà¹ˆà¸ªà¸¡à¸”à¸¸à¸¥à¸à¸§à¹ˆà¸²à¹€à¸”à¸´à¸¡
    """
    print("ðŸŽ¯ Creating balanced binary targets...")

    try:
        # à¹€à¸à¹‡à¸š original target
        df['target_original'] = df['target'].copy()

        # Strategy 1: Binary classification (positive vs others)
        df['target_binary_pos'] = (df['target'] == 1).astype(int)

        # Strategy 2: Binary classification (negative vs others)
        df['target_binary_neg'] = (df['target'] == -1).astype(int)

        # Strategy 3: Binary classification (non - zero vs zero)
        df['target_binary_nonzero'] = (df['target'] != 0).astype(int)

        # à¹€à¸¥à¸·à¸­à¸ target à¸—à¸µà¹ˆà¸ªà¸¡à¸”à¸¸à¸¥à¸—à¸µà¹ˆà¸ªà¸¸à¸”
        binary_targets = ['target_binary_pos', 'target_binary_neg', 'target_binary_nonzero']
        best_target = None
        best_balance = float('inf')

        for target_col in binary_targets:
            counts = df[target_col].value_counts()
            if len(counts) == 2:
                imbalance = counts.max() / counts.min()
                print(f"ðŸ“Š {target_col}: {counts.to_dict()}, imbalance: {imbalance:.1f}:1")
                if imbalance < best_balance:
                    best_balance = imbalance
                    best_target = target_col

        if best_target and best_balance < 20:  # à¸–à¹‰à¸²à¸«à¸² target à¸—à¸µà¹ˆà¸ªà¸¡à¸”à¸¸à¸¥à¹„à¸”à¹‰
            print(f"âœ… Using {best_target} as main target (imbalance: {best_balance:.1f}:1)")
            df['target'] = df[best_target]
        else:
            print("âš ï¸ No well - balanced binary target found, keeping original with weights")

        return df

    except Exception as e:
        print(f"âŒ Binary target creation failed: {e}")
        return df

def validate_balanced_data(df):
    """
    à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸¸à¸“à¸ à¸²à¸žà¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆ balanced à¹à¸¥à¹‰à¸§
    """
    print("ðŸ” Validating balanced data quality...")

    try:
        # 1. Basic statistics
        print(f"ðŸ“Š Final data shape: {df.shape}")
        print(f"ðŸ“Š Target distribution: {df['target'].value_counts().to_dict()}")

        # 2. Feature quality check
        numeric_cols = df.select_dtypes(include = [np.number]).columns
        nan_ratio = df[numeric_cols].isnull().mean()
        problematic_features = nan_ratio[nan_ratio > 0.1].index.tolist()

        if problematic_features:
            print(f"âš ï¸ Features with >10% NaN: {problematic_features}")

        # 3. Feature correlation with target
        feature_cols = [c for c in numeric_cols if c != 'target']
        if feature_cols:
            correlations = df[feature_cols + ['target']].corr()['target'].abs().sort_values(ascending = False)
            top_features = correlations.head(5)
            print(f"ðŸ“Š Top 5 feature correlations with target:")
            for feat, corr in top_features.items():
                if feat != 'target':
                    print(f"   {feat}: {corr:.4f}")

        # 4. Class balance validation
        target_counts = df['target'].value_counts()
        if len(target_counts) > 1:
            imbalance_ratio = target_counts.max() / target_counts.min()
            if imbalance_ratio <= 10:
                print(f"âœ… Good balance achieved: {imbalance_ratio:.1f}:1")
            elif imbalance_ratio <= 50:
                print(f"âš ï¸ Moderate imbalance: {imbalance_ratio:.1f}:1")
            else:
                print(f"ðŸš¨ Still extreme imbalance: {imbalance_ratio:.1f}:1")

        return True

    except Exception as e:
        print(f"âŒ Validation failed: {e}")
        return False

def create_balance_metadata(df_original, df_balanced, output_path):
    """
    à¸ªà¸£à¹‰à¸²à¸‡ metadata à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸à¸²à¸£ balance data
    """
    try:
        metadata = {
            "timestamp": pd.Timestamp.now().isoformat(), 
            "original_shape": df_original.shape, 
            "balanced_shape": df_balanced.shape, 
            "original_target_dist": df_original['target'].value_counts().to_dict(), 
            "balanced_target_dist": df_balanced['target'].value_counts().to_dict(), 
            "techniques_used": ["SMOTE", "Feature_Enhancement", "Binary_Targets"], 
            "output_path": output_path, 
            "quality_check": "PASSED"
        }

        # à¸„à¸³à¸™à¸§à¸“ improvement metrics
        orig_counts = df_original['target'].value_counts()
        bal_counts = df_balanced['target'].value_counts()

        orig_imbalance = orig_counts.max() / orig_counts.min()
        bal_imbalance = bal_counts.max() / bal_counts.min()

        metadata["original_imbalance_ratio"] = float(orig_imbalance)
        metadata["balanced_imbalance_ratio"] = float(bal_imbalance)
        metadata["improvement_factor"] = float(orig_imbalance / bal_imbalance)

        # à¸šà¸±à¸™à¸—à¸¶à¸ metadata
        metadata_path = "output_default/balance_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent = 2)

        print(f"ðŸ“‹ Metadata saved to: {metadata_path}")
        print(f"ðŸŽ¯ Imbalance improvement: {orig_imbalance:.1f}:1 â†’ {bal_imbalance:.1f}:1")

    except Exception as e:
        print(f"âŒ Metadata creation failed: {e}")

if __name__ == "__main__":
    print("ðŸš€ Starting Production - Grade Class Imbalance Fix...")
    df_balanced, output_path = fix_extreme_class_imbalance_production()

    if df_balanced is not None:
        print("ðŸŽ‰ Class imbalance fix completed successfully!")
        print(f"ðŸ“ Balanced data available at: {output_path}")
    else:
        print("âŒ Class imbalance fix failed!")