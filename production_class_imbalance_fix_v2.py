        from projectp.utils_feature import ensure_super_features_file
from sklearn.utils import resample
from typing import Tuple, Optional, Dict, Any, Union
import json
import logging
import numpy as np
import os
import pandas as pd
            import traceback
import warnings
"""
üöÄ PRODUCTION CLASS IMBALANCE FIX v2.0
 =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = 
‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Class Imbalance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production Environment
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Extreme Imbalance (>200:1) ‡πÅ‡∏•‡∏∞ Single Class

Features:
- Intelligent sampling with noise injection
- Robust fallback mechanisms
- Production - grade error handling
- Comprehensive logging and validation

Author: NICEGOLD AI System
Version: 2.0 (Production Ready)
"""


# Configure logging
logging.basicConfig(level = logging.INFO)
logger = logging.getLogger(__name__)

# Suppress warnings
warnings.filterwarnings("ignore", category = UserWarning)
warnings.filterwarnings("ignore", category = FutureWarning)


class ProductionClassBalancer:
    """Production - grade class imbalance handler."""

    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        self.metadata = {}

    def apply_comprehensive_fix(self, df: pd.DataFrame, target_col: str = 'target') -> pd.DataFrame:
        """Apply comprehensive class imbalance fix."""
        logger.info("üöÄ PRODUCTION CLASS IMBALANCE FIX STARTING...")

        try:
            # Store original info
            original_shape = df.shape
            original_distribution = df[target_col].value_counts().to_dict()

            logger.info(f"üìä Original data shape: {original_shape}")
            logger.info(f"üìä Original target distribution: {original_distribution}")

            # Check imbalance severity
            imbalance_ratio = self._calculate_imbalance_ratio(df[target_col])
            logger.info(f"‚öñÔ∏è Imbalance ratio: {imbalance_ratio:.1f}:1")

            if imbalance_ratio > 100:
                logger.warning("üö® EXTREME IMBALANCE DETECTED - Applying comprehensive fixes...")
                df_fixed = self._handle_extreme_imbalance(df, target_col)
            elif imbalance_ratio > 20:
                logger.info("‚ö†Ô∏è High imbalance detected - Applying standard fixes...")
                df_fixed = self._handle_high_imbalance(df, target_col)
            else:
                logger.info("‚úÖ Imbalance is manageable - Applying light fixes...")
                df_fixed = self._handle_light_imbalance(df, target_col)

            # Validate results
            self._validate_results(df_fixed, target_col, original_shape)

            # Store metadata
            self._save_metadata(df_fixed, target_col, original_shape, original_distribution)

            return df_fixed

        except Exception as e:
            logger.error(f"‚ùå Class imbalance fix failed: {e}")
            traceback.print_exc()
            return df

    def _calculate_imbalance_ratio(self, target_series: pd.Series) -> float:
        """Calculate imbalance ratio safely."""
        try:
            counts = target_series.value_counts()
            if len(counts) <= 1:
                return 1.0
            return counts.max() / counts.min()
        except:
            return 1.0

    def _handle_extreme_imbalance(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Handle extreme imbalance (>100:1)."""
        logger.info("üéØ Applying advanced sampling techniques...")

        try:
            # Step 1: Create more balanced binary targets
            df_balanced = self._create_balanced_binary_targets(df, target_col)

            # Step 2: Intelligent sampling
            df_sampled = self._apply_intelligent_sampling(df_balanced, target_col)

            # Step 3: Feature enhancement for minority classes
            df_enhanced = self._enhance_minority_features(df_sampled, target_col)

            return df_enhanced

        except Exception as e:
            logger.error(f"‚ùå Extreme imbalance handling failed: {e}")
            return self._fallback_sampling(df, target_col)

    def _handle_high_imbalance(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Handle high imbalance (20 - 100:1)."""
        try:
            # Apply moderate sampling
            return self._apply_moderate_sampling(df, target_col)
        except:
            return self._fallback_sampling(df, target_col)

    def _handle_light_imbalance(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Handle light imbalance (<20:1)."""
        try:
            # Apply light adjustments
            return self._apply_light_adjustments(df, target_col)
        except:
            return df

    def _create_balanced_binary_targets(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Create more balanced binary targets."""
        logger.info("üéØ Creating balanced binary targets...")

        try:
            df_new = df.copy()

            # Store original target
            df_new[f'{target_col}_original'] = df_new[target_col]

            # Strategy 1: Non - zero vs Zero
            target_nonzero = (df_new[target_col] != 0).astype(int)
            counts_nonzero = pd.Series(target_nonzero).value_counts()

            if len(counts_nonzero) == 2:
                imbalance_nonzero = counts_nonzero.max() / counts_nonzero.min()
                logger.info(f"üìä Non - zero target imbalance: {imbalance_nonzero:.1f}:1")

                if imbalance_nonzero < 20:
                    df_new[target_col] = target_nonzero
                    logger.info("‚úÖ Using non - zero vs zero target")
                    return df_new

            # Strategy 2: Quantile - based binary target
            numeric_cols = df_new.select_dtypes(include = [np.number]).columns
            if len(numeric_cols) > 0:
                main_col = numeric_cols[0]  # Use first numeric column
                median_val = df_new[main_col].median()
                df_new[target_col] = (df_new[main_col] > median_val).astype(int)
                logger.info(f"‚úÖ Using {main_col} - based binary target")

            return df_new

        except Exception as e:
            logger.error(f"‚ùå Binary target creation failed: {e}")
            return df

    def _apply_intelligent_sampling(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Apply intelligent sampling with noise injection."""
        logger.info("üß† Applying intelligent sampling...")

        try:
            target_counts = df[target_col].value_counts()
            majority_class = target_counts.idxmax()
            minority_classes = [c for c in target_counts.index if c != majority_class]

            # Calculate target sizes
            majority_size = target_counts[majority_class]
            target_minority_size = min(majority_size // 5, 50000)  # Max 20% of majority

            balanced_parts = []

            # Handle majority class (downsample if too large)
            majority_df = df[df[target_col] == majority_class]
            if len(majority_df) > 200000:
                majority_df = majority_df.sample(n = 200000, random_state = self.random_state)
            balanced_parts.append(majority_df)

            # Handle minority classes
            for cls in minority_classes:
                minority_df = df[df[target_col] == cls]
                current_size = len(minority_df)

                if current_size < target_minority_size:
                    # Oversample with noise
                    boosted_df = self._oversample_with_noise(
                        minority_df, target_minority_size, cls
                    )
                    balanced_parts.append(boosted_df)
                else:
                    balanced_parts.append(minority_df)

            # Combine and shuffle
            df_balanced = pd.concat(balanced_parts, ignore_index = True)
            df_balanced = df_balanced.sample(frac = 1, random_state = self.random_state).reset_index(drop = True)

            logger.info(f"‚úÖ Intelligent sampling completed: {df_balanced.shape}")
            logger.info(f"üìä New distribution: {df_balanced[target_col].value_counts().to_dict()}")

            return df_balanced

        except Exception as e:
            logger.error(f"‚ùå Intelligent sampling failed: {e}")
            return df

    def _oversample_with_noise(self, df: pd.DataFrame, target_size: int, class_label: Any) -> pd.DataFrame:
        """Oversample minority class with intelligent noise injection."""
        try:
            current_size = len(df)
            if current_size >= target_size:
                return df

            replications_needed = target_size // current_size
            remainder = target_size % current_size

            boosted_parts = [df]  # Original data

            # Add replications with increasing noise
            for rep in range(replications_needed):
                noise_factor = 0.01 + 0.005 * rep  # Increasing noise
                noisy_copy = self._add_intelligent_noise(df, noise_factor)
                boosted_parts.append(noisy_copy)

            # Add remainder
            if remainder > 0:
                remainder_sample = df.sample(n = remainder, random_state = self.random_state)
                remainder_noisy = self._add_intelligent_noise(remainder_sample, 0.005)
                boosted_parts.append(remainder_noisy)

            return pd.concat(boosted_parts, ignore_index = True)

        except Exception as e:
            logger.error(f"‚ùå Oversampling failed for class {class_label}: {e}")
            return df

    def _add_intelligent_noise(self, df: pd.DataFrame, noise_factor: float = 0.01) -> pd.DataFrame:
        """Add intelligent noise to numeric columns."""
        try:
            df_noisy = df.copy()
            numeric_cols = df_noisy.select_dtypes(include = [np.number]).columns

            # Exclude target and ID columns
            noise_cols = [c for c in numeric_cols if not any(
                keyword in c.lower() for keyword in ['target', 'id', 'index', 'key']
            )]

            for col in noise_cols:
                if df_noisy[col].std() > 0:
                    noise = np.random.normal(
                        0, df_noisy[col].std() * noise_factor, len(df_noisy)
                    )
                    df_noisy[col] = df_noisy[col] + noise

            return df_noisy

        except Exception as e:
            logger.error(f"‚ùå Noise addition failed: {e}")
            return df

    def _enhance_minority_features(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Enhance features to boost minority class signals."""
        logger.info("üì° Enhancing minority class features...")

        try:
            target_counts = df[target_col].value_counts()
            if len(target_counts) <= 1:
                return df

            minority_classes = target_counts[target_counts < target_counts.max()].index

            # Add minority indicator features
            for cls in minority_classes:
                df[f'is_minority_{cls}'] = (df[target_col] == cls).astype(int)

            # Add distance features for top numeric columns
            numeric_cols = df.select_dtypes(include = [np.number]).columns
            feature_cols = [c for c in numeric_cols if c != target_col and not c.startswith('is_minority_')][:5]

            for cls in minority_classes:
                minority_mask = df[target_col] == cls
                if minority_mask.sum() > 0:
                    for col in feature_cols:
                        minority_mean = df.loc[minority_mask, col].mean()
                        df[f'{col}_dist_to_minority_{cls}'] = np.abs(df[col] - minority_mean)

            logger.info("‚úÖ Minority feature enhancement completed")
            return df

        except Exception as e:
            logger.error(f"‚ùå Minority feature enhancement failed: {e}")
            return df

    def _apply_moderate_sampling(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Apply moderate sampling for high imbalance."""
        try:
            target_counts = df[target_col].value_counts()
            majority_class = target_counts.idxmax()
            minority_classes = [c for c in target_counts.index if c != majority_class]

            balanced_parts = [df[df[target_col] == majority_class]]

            for cls in minority_classes:
                minority_df = df[df[target_col] == cls]
                current_size = len(minority_df)
                target_size = min(target_counts[majority_class] // 10, current_size * 3)

                if current_size < target_size:
                    upsampled = resample(
                        minority_df, 
                        replace = True, 
                        n_samples = target_size, 
                        random_state = self.random_state
                    )
                    balanced_parts.append(upsampled)
                else:
                    balanced_parts.append(minority_df)

            return pd.concat(balanced_parts, ignore_index = True)

        except Exception as e:
            logger.error(f"‚ùå Moderate sampling failed: {e}")
            return df

    def _apply_light_adjustments(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Apply light adjustments for manageable imbalance."""
        try:
            # Just add some features to help with slight imbalance
            numeric_cols = df.select_dtypes(include = [np.number]).columns
            if len(numeric_cols) > 1:
                main_col = [c for c in numeric_cols if c != target_col][0]
                df[f'{main_col}_rank'] = df[main_col].rank(pct = True)

            return df

        except:
            return df

    def _fallback_sampling(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """Fallback sampling when all else fails."""
        logger.warning("üîÑ Applying fallback sampling...")

        try:
            # Simple random oversampling
            target_counts = df[target_col].value_counts()
            if len(target_counts) <= 1:
                return df

            max_count = target_counts.max()
            balanced_parts = []

            for cls in target_counts.index:
                class_df = df[df[target_col] == cls]
                current_count = len(class_df)
                target_count = min(max_count, current_count * 2)  # At most double

                if current_count < target_count:
                    upsampled = resample(
                        class_df, 
                        replace = True, 
                        n_samples = target_count, 
                        random_state = self.random_state
                    )
                    balanced_parts.append(upsampled)
                else:
                    balanced_parts.append(class_df)

            return pd.concat(balanced_parts, ignore_index = True)

        except Exception as e:
            logger.error(f"‚ùå Fallback sampling failed: {e}")
            return df

    def _validate_results(self, df: pd.DataFrame, target_col: str, original_shape: Tuple[int, int]) -> None:
        """Validate the results of class balancing."""
        try:
            new_shape = df.shape
            new_distribution = df[target_col].value_counts().to_dict()

            logger.info(f"üìä Final shape: {new_shape}")
            logger.info(f"üìä Final distribution: {new_distribution}")

            # Check for improvement
            new_imbalance = self._calculate_imbalance_ratio(df[target_col])
            logger.info(f"‚öñÔ∏è Final imbalance ratio: {new_imbalance:.1f}:1")

            if new_imbalance <= 10:
                logger.info("‚úÖ Excellent balance achieved!")
            elif new_imbalance <= 50:
                logger.info("‚úÖ Good balance achieved!")
            else:
                logger.warning("‚ö†Ô∏è Still imbalanced, but improved")

        except Exception as e:
            logger.error(f"‚ùå Validation failed: {e}")

    def _save_metadata(self, df: pd.DataFrame, target_col: str, 
                      original_shape: Tuple[int, int], original_distribution: Dict) -> None:
        """Save metadata about the balancing process."""
        try:
            os.makedirs('output_default', exist_ok = True)

            metadata = {
                'original_shape': original_shape, 
                'final_shape': df.shape, 
                'original_distribution': original_distribution, 
                'final_distribution': df[target_col].value_counts().to_dict(), 
                'original_imbalance_ratio': self._calculate_imbalance_ratio(pd.Series(list(original_distribution.values()))), 
                'final_imbalance_ratio': self._calculate_imbalance_ratio(df[target_col]), 
                'improvement_ratio': None
            }

            if metadata['original_imbalance_ratio'] > 0:
                metadata['improvement_ratio'] = metadata['original_imbalance_ratio'] / metadata['final_imbalance_ratio']

            with open('output_default/balance_metadata.json', 'w') as f:
                json.dump(metadata, f, indent = 2)

            logger.info("‚úÖ Metadata saved to output_default/balance_metadata.json")

        except Exception as e:
            logger.error(f"‚ùå Metadata saving failed: {e}")


def fix_extreme_class_imbalance_production(df: pd.DataFrame, target_col: str = 'target') -> pd.DataFrame:
    """
    üöÄ Production - grade class imbalance fix for extreme cases (>200:1).

    Args:
        df: Input DataFrame
        target_col: Target column name

    Returns:
        DataFrame with balanced classes
    """
    balancer = ProductionClassBalancer()
    return balancer.apply_comprehensive_fix(df, target_col)


def main():
    """Main function for standalone testing."""
    print("üöÄ PRODUCTION CLASS IMBALANCE FIX v2.0")
    print(" = " * 50)

    # Test with sample data
    try:

        print("üìä Loading test data...")
        fe_super_path = ensure_super_features_file()
        df = pd.read_parquet(fe_super_path)

        print(f"Original shape: {df.shape}")
        print(f"Original target distribution: {df['target'].value_counts().to_dict()}")

        # Apply fix
        df_fixed = fix_extreme_class_imbalance_production(df)

        # Save result
        output_path = "output_default/balanced_data_production.parquet"
        df_fixed.to_parquet(output_path)
        print(f"‚úÖ Fixed data saved to: {output_path}")

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()