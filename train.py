from colorama import Fore, Style, init as colorama_init
from real_data_loader import load_real_trading_data
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score
from typing import Tuple, Optional, Dict, Any
        import joblib
        import json
import numpy as np
import os
import pandas as pd
        import traceback
import warnings
"""
Training Module - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
 =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = 

‡∏´‡πâ‡∏≤‡∏°:
- ‡πÉ‡∏ä‡πâ dummy data
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á

‡πÉ‡∏ä‡πâ:
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• XAUUSD ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- Real market data ‡∏à‡∏≤‡∏Å XAUUSD_M1.csv ‡πÅ‡∏•‡∏∞ XAUUSD_M15.csv
"""

warnings.filterwarnings('ignore')

# ‡πÉ‡∏ä‡πâ real_data_loader ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
colorama_init(autoreset = True)

def load_only_real_data(timeframe: str = "M15", max_rows: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô - ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ dummy data

    Args:
        timeframe: "M1" ‡∏´‡∏£‡∏∑‡∏≠ "M15" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        max_rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (None = ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)

    Returns:
        DataFrame ‡πÅ‡∏•‡∏∞ metadata ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
    """
    print(f"{Fore.GREEN}üîÑ Loading REAL MARKET DATA ONLY (timeframe: {timeframe}){Style.RESET_ALL}")
    print(f"{Fore.RED}‚ùå NO DUMMY DATA ALLOWED{Style.RESET_ALL}")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    data_files = {
        "M1": "XAUUSD_M1.csv", 
        "M15": "XAUUSD_M15.csv"
    }

    expected_file = data_files.get(timeframe.upper())
    if not expected_file or not os.path.exists(expected_file):
        raise FileNotFoundError(f"‚ùå Real data file not found: {expected_file}")

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    df, info = load_real_trading_data(
        timeframe = timeframe, 
        max_rows = max_rows, 
        profit_threshold = 0.12
    )

    # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
    info['data_validation'] = {
        'is_real_data': True, 
        'is_dummy_data': False, 
        'is_synthetic_data': False, 
        'source_verified': True, 
        'file_source': expected_file
    }

    print(f"{Fore.GREEN}‚úÖ VERIFIED: Using real market data only{Style.RESET_ALL}")
    print(f"{Fore.GREEN}üìä Data points: {len(df):, } (100% real market data){Style.RESET_ALL}")

    return df, info

def validate_real_data_only(df: pd.DataFrame, info: Dict[str, Any]) -> bool:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    """
    validation = info.get('data_validation', {})

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô
    is_real = validation.get('is_real_data', False)
    is_not_dummy = not validation.get('is_dummy_data', True)
    is_not_synthetic = not validation.get('is_synthetic_data', True)
    source_verified = validation.get('source_verified', False)

    if not all([is_real, is_not_dummy, is_not_synthetic, source_verified]):
        raise ValueError("‚ùå VALIDATION FAILED: Not verified as real data!")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    if len(df) < 1000:
        print(f"{Fore.YELLOW}‚ö†Ô∏è WARNING: Limited data points ({len(df):, }){Style.RESET_ALL}")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏• (‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥)
    price_range = df['Close'].max() - df['Close'].min()
    if price_range < 100:  # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≠‡∏á‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ $100
        print(f"{Fore.YELLOW}‚ö†Ô∏è WARNING: Unusual price range: ${price_range:.2f}{Style.RESET_ALL}")

    print(f"{Fore.GREEN}‚úÖ DATA VALIDATION PASSED: Real market data confirmed{Style.RESET_ALL}")
    return True

def train(timeframe: str = "M15", 
          max_rows: Optional[int] = None, 
          model_type: str = "random_forest") -> Dict[str, Any]:
    """
    ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

    Args:
        timeframe: "M1" ‡∏´‡∏£‡∏∑‡∏≠ "M15"
        max_rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (None = ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)
        model_type: "random_forest" ‡∏´‡∏£‡∏∑‡∏≠ "gradient_boosting"

    Returns:
        ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    """
    print(f"{Fore.CYAN}{' = '*80}")
    print(f"{Fore.CYAN}ü§ñ TRAINING WITH REAL DATA ONLY")
    print(f"{Fore.CYAN}{' = '*80}")

    try:
        # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        df, info = load_only_real_data(timeframe = timeframe, max_rows = max_rows)

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        validate_real_data_only(df, info)

        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML
        print(f"{Fore.GREEN}üîß Preparing features from real data...{Style.RESET_ALL}")

        # ‡πÅ‡∏¢‡∏Å features ‡πÅ‡∏•‡∏∞ targets
        feature_cols = [col for col in df.columns if col not in
                       ['Time', 'target', 'target_binary', 'target_return']]

        X = df[feature_cols].select_dtypes(include = [np.number])
        y = df['target']

        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        X = X.fillna(X.median())  # ‡πÉ‡∏ä‡πâ median ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á

        print(f"{Fore.GREEN}üìä Features: {X.shape[1]} columns, {X.shape[0]:, } samples{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üéØ Target distribution: {y.value_counts().to_dict()}{Style.RESET_ALL}")

        # 4. ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size = 0.2, random_state = 42, stratify = y
        )

        print(f"{Fore.GREEN}üîß Training set: {X_train.shape[0]:, } samples{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üîß Test set: {X_test.shape[0]:, } samples{Style.RESET_ALL}")

        # 5. ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        print(f"{Fore.GREEN}ü§ñ Training {model_type} model...{Style.RESET_ALL}")

        if model_type == "random_forest":
            model = RandomForestClassifier(
                n_estimators = 100, 
                max_depth = 10, 
                random_state = 42, 
                n_jobs = -1
            )
        elif model_type == "gradient_boosting":
            model = GradientBoostingClassifier(
                n_estimators = 100, 
                max_depth = 6, 
                random_state = 42
            )
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model.fit(X_train, y_train)

        # 6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
        print(f"{Fore.GREEN}üìà Evaluating model performance...{Style.RESET_ALL}")

        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)

        # Cross - validation
        cv_scores = cross_val_score(model, X, y, cv = 5, scoring = 'accuracy')

        # Feature importance (top 10)
        feature_importance = pd.DataFrame({
            'feature': X.columns, 
            'importance': model.feature_importances_
        }).sort_values('importance', ascending = False).head(10)

        print(f"{Fore.GREEN}üìä Training Accuracy: {train_score:.3f}{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üìä Test Accuracy: {test_score:.3f}{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üìä CV Accuracy: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}{Style.RESET_ALL}")

        print(f"{Fore.CYAN}üîù Top Features:{Style.RESET_ALL}")
        for idx, row in feature_importance.iterrows():
            print(f"   {row['feature']}: {row['importance']:.3f}")

        # 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        os.makedirs("output_default", exist_ok = True)

        results = {
            'model_type': model_type, 
            'timeframe': timeframe, 
            'data_source': 'REAL_MARKET_DATA_ONLY', 
            'samples_total': len(df), 
            'samples_train': len(X_train), 
            'samples_test': len(X_test), 
            'features_count': X.shape[1], 
            'training_accuracy': float(train_score), 
            'test_accuracy': float(test_score), 
            'cv_accuracy_mean': float(cv_scores.mean()), 
            'cv_accuracy_std': float(cv_scores.std()), 
            'feature_importance': feature_importance.to_dict('records'), 
            'data_validation': info['data_validation'], 
            'target_distribution': y.value_counts().to_dict()
        }

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å model ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        model_filename = f"output_default/model_{model_type}_{timeframe}_real_data.joblib"
        joblib.dump(model, model_filename)

        with open("output_default/training_results_real_data.json", "w") as f:
            json.dump(results, f, indent = 2)

        print(f"{Fore.GREEN}üíæ Model saved: {model_filename}{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üìÅ Results saved: output_default/training_results_real_data.json{Style.RESET_ALL}")

        # 8. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        print(f"\n{Fore.CYAN}{' = '*80}")
        print(f"{Fore.CYAN}üìä TRAINING SUMMARY (REAL DATA ONLY)")
        print(f"{Fore.CYAN}{' = '*80}")
        print(f"{Fore.GREEN}‚úÖ Data Source: 100% Real Market Data")
        print(f"{Fore.GREEN}‚úÖ Model: {model_type}")
        print(f"{Fore.GREEN}‚úÖ Timeframe: {timeframe}")
        print(f"{Fore.GREEN}‚úÖ Samples: {len(df):, }")
        print(f"{Fore.GREEN}‚úÖ Test Accuracy: {test_score:.3f}")
        print(f"{Fore.GREEN}‚úÖ No dummy data used")
        print(f"{Fore.CYAN}{' = '*80}{Style.RESET_ALL}")

        return results

    except Exception as e:
        print(f"{Fore.RED}‚ùå Training failed: {e}{Style.RESET_ALL}")
        raise

def train_multi_timeframe(max_rows: Optional[int] = None) -> Dict[str, Any]:
    """
    ‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ timeframe
    """
    print(f"{Fore.CYAN}üî• MULTI - TIMEFRAME TRAINING (REAL DATA ONLY){Style.RESET_ALL}")

    results = {}

    for timeframe in ["M1", "M15"]:
        try:
            print(f"\n{Fore.YELLOW}üîÑ Training {timeframe} model...{Style.RESET_ALL}")
            result = train(timeframe = timeframe, max_rows = max_rows)
            results[timeframe] = result

        except Exception as e:
            print(f"{Fore.RED}‚ùå Failed to train {timeframe}: {e}{Style.RESET_ALL}")
            results[timeframe] = {'error': str(e)}

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏£‡∏ß‡∏°
    with open("output_default/multi_timeframe_results_real_data.json", "w") as f:
        json.dump(results, f, indent = 2)

    print(f"{Fore.GREEN}üíæ Multi - timeframe results saved{Style.RESET_ALL}")
    return results

if __name__ == "__main__":
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    print("üöÄ Starting real data training...")

    try:
        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö single timeframe
        print("Training with M15 timeframe...")
        result = train(timeframe = "M15", max_rows = 10000)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 10k ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        print("‚úÖ Training completed successfully!")

        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö multi - timeframe (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        # print("Training multi - timeframe...")
        # multi_result = train_multi_timeframe(max_rows = 5000)

    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        traceback.print_exc()